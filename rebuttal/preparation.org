
Thanks very much for the thoughtful comments.
Thank you for the thoughtful reviews.

* Reviewer #1 (Accept)

#+begin_quote
Due to some design decisions of Latplan and how NNs work, the resulting
propositional representations could have problems related to stability
#+end_quote

I think his confidence is a bit lower

#+begin_quote
the representations generated in two time steps could differ due to some
stochasticity in the learning procedure
#+end_quote

"two time steps" -> unsure about what he implies, it is for single time step
Also, stochasticity prevails after the learning procedure too


#+begin_quote
As a detailed comment, you should explain ARM_2 when it is first
referenced in the Introduction.
#+end_quote

yes

#+begin_quote
When you describe Latplan in Sec 2, given that it does not get as
input labels for actions, should we assume Latplan generates a
completely instantiated domain?
#+end_quote

not sure what s/he means by "instantiated domain"

yes for AMA1

AMA2 does not return a PDDL, but a black box neural action model.


#+begin_quote
You assume b_n to be independent in Sec. 4. It is clear that it
greatly simplifies the math. But, does it have any implication in the
results? As far as I understand your work in terms of planning,
propositions are not usually (or necessarily) independent.
#+end_quote

(not sure)


* Reviewer #2 (Accept)

#+begin_quote
the base paper (Asai and Fukunaga 2018) tackles this issue already in a
first way and now the authors suggest an additional regularization.
#+end_quote

The base paper did not explain the Entropy Regularization, a diversion from
the regular Gumbel-Softmax VAE.
Thus, the base paper did not address the stability issue, only their implementation did.

#+begin_quote
In table 1, middle
column about MSE, the authors speak about the orders of magnitude larger MSE for
N=36, but for N=100/1000 the same happens in the LightsOut domain (which
interestingly was not problematic for N=36). Do you have an explanation
for this?
#+end_quote

Typically, MSE below 1.0e-3 is visually not significant to human (both 2.8e-14, 1.2e-5).

#+begin_quote
In the original SAE the Kullback-Leibler divergence helps stabilizing the latent
representation of the state. Your ZSAE uses both the KL divergence and your own
regularization. Have you tested/Can you test what the effect of your own
regularization alone is?
#+end_quote

no... (should we start this experiment)

#+begin_quote
The way you have written down your own regularization allows the latent
representation to be non binary. What are your thoughts about using non binary
predicates (like in SAS+ representation a variable can have multiple values)?
#+end_quote

It was intentionally written so with SAS+ in mind
(e.g. if k \in {0,1,2}, it is a variable with 3 values).
The base paper (Asai 2018) also mentions SAS+.
The expressivity of the representation is not affected by limiting the domain to
binary values (as STRIPS and SAS+ is equivalent).
However, hand-coding the number of possible values for each variable
would require human effort.

#+begin_quote
In Section 6.1 your have written that because of the probabilistic nature of the
latent representation you encoded the same image 100 times and took the mean. In
my understanding the mean would lead you to have continous values whereas the
system later operates on 0 and 1.
#+end_quote

We encoded the same image 100 times and took the *variance*.
Then, for the 100 *different* images, we took the mean of the variance.

Also, the value is just a metric for measuring the stability of the NN in this experiment.
this value is not used in the later NN pipeline and the system.

#+begin_quote
(This would also be a question in LatPlan) In a real world setting without a
ground truth to check for, do you have an idea how to select N correctly?
#+end_quote

After the training, the correctness of the SAE is checked by
applying the SAE to an unseen set of images (test instances)
and checking the error between the input and the reconstrcution.
Since the input (raw observation) is the ground truth by itself,
we can tell that the NN is not learning if the error is large.

With the vanilla SAE, you have to rely on try-and-errors to find the best N.
If N is too small, the network cannot represent the world.
Too large N also causes the stability issues.

With ZSAE, we can set N very large and let the zero-suppression reduce the
number of effective bits automatically.

In practice, the size of N would be restricted by the hardware limitation (GPU)
and the time constraint (large network = slow training).
