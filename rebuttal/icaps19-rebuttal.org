#+TITLE: 
#+DATE: 
#+AUTHOR: 
#+EMAIL: 
#+OPTIONS: ':nil *:t -:t ::t <:t H:3 \n:nil ^:t arch:headline author:nil
#+OPTIONS: c:nil creator:nil d:(not "LOGBOOK") date:nil e:t email:nil
#+OPTIONS: f:t inline:t num:t p:nil pri:nil stat:t tags:t tasks:t tex:t
#+OPTIONS: timestamp:nil toc:nil todo:t |:t
#+CREATOR: Emacs 24.3.1 (Org mode 8.2.10)
#+DESCRIPTION:
#+EXCLUDE_TAGS: noexport
#+KEYWORDS:
#+LANGUAGE: en
#+SELECT_TAGS: export



Thank you for the thoughtful reviews.
All minor issues reported in the reviews will be revised.
In the following, we abbreviate Reviewer #X to RX.
Individual responses come after the overall response.

* R1

# Significance: 2: (modest contribution or average impact)
# Soundness: 3: (correct)
# Scholarship: 2: (relevant literature cited but could be expanded)
# Clarity: 3: (well organized and well written)
# Reproducibility: 3: (authors describe the implementation and domains in sufficient detail)
# Overall evaluation: 2: (accept)
# Review:

# This paper extends an existing approach for learning symbolic state representations in planning domains (well.. that is what it is used for, but it coule be used for other things). The idea is to put more constraints on an auto-encoder network setup such that the latent space is forced into a more "stable" bit representation. The authors define what that means and contribute the algorithm, an analysis of the previous algorithm (using a novel viewpoint) and the introduction of the symbol stability problem. Many experiments are included to test various aspects and to compare to two previous approaches.
# 
# This paper is well-written, focused and it contains insightful experiments for what the authors claim to contribute. It is interesting to see that in addition to a new algorithm, the authors also analyze the original algorithm and find out (confirmed by contact with the original authors) that even the original algorithm was different from its description.
# 
# This paper is about an important problem: with all the deep learning success, it is good to look at how such models can be used to obtain representations that are useful for (symbolic) planning, and especially how we can obtain stable representations. The problem setting is very clear from the start, all the sub-steps and problems are well introduced and also covered in the experiments, and terminology is clear throughout the paper. Most of the questions I had while reading were answered right away or through the experiments. The first half of the paper could use a more extensive example to get hands-on with the problem of stability; I agree that the pictures do introduce it, but on a slightly more abstract level though. Some of the language can be improved (some small things like literals missing, but overall the paper is quite polished already). Figure 4 is not very clear (compared to the rest of the paper).
# 
# Section 3 might overdo it a little when explaining things related to the main theme of the paper; I guess some of it is redundant.

# I think that all experiments "before" the actual planning tests are insightful and convincing (also the comparisons).

# For the planning experiments themselves, I think these are not overly convincing.
> I do see the effects of the new regularization on the latent representation, and the effect on planning,
> but these are not too large if we only look at success (Table 3, right).

We discussed that the source of stochasticity is either internal or external,
and that argmax supresses the internal stochasticity, and the regularization
supresses the external one. (Sec.3,par.1)

**The main result is therefore the evaluation with the noisy input**, which is of the practical importance,
not the clean input, which lacks external stochasticity.

Notice that for the strong gaussian noise (Ïƒ=0.6 for pixel values between 0.0 and 1.0), the number of instances solved with AMA2 have more than doubled. The noise is so strong that we hardly recognize the original content (e.g. mandrill, numbers) in the image (see [anonymous image link]), but the system is able to map it to the appropriate propositional state and solves it successfully.

Also, note that N=36 for SAE is a result of hyperparameter search, thus comparing this best case with a ZSAE with different N is unfair.

# The number of solved instances is almost the same, but according to the end of section 6.3. search efforts and runtimes do differ, but I think more experiments/analysis is needed here.
# This is the only weaker point of the paper, since it is the main focus (seeing how better representations enable "better" planning).
# I also feel that if one leaves the planning domain aside, the experimental section could have appealed to other methods too that work on compression of (auto-encoder based) learning.
# The related work could also be expanded somewhat if looking more in this direction.
# 
# Nevertheless, this is a nice paper with interesting results.

* R2

# Significance: 1: (minimal contribution or weak impact) Minor extension of an already-published method
# Soundness: 2: (minor inconsistencies or small fixable errors)
# Scholarship: 2: (relevant literature cited but could be expanded)
# Clarity: 2: (mostly readable with some room for improvement)
# 
# Many important details are not described precisely. Understanding the system requires reading the earlier LatPlan paper (Asai and Fukunaga, 2018), which is itself difficult to parse.
# 
# Reproducibility: 	
# 3: (authors describe the implementation and domains in sufficient detail)
# Would be very difficult to reproduce from this paper alone, but the work is an extension of the LatPlan system, which has available source code.
# 
# Overall evaluation: 	
# -1: (weak reject)
# 
# Review: 	Summary:

# The paper proposes an extension to the LatPlan system (Asai and Fukunaga, 2018) to improve the "stability" of the learned discrete state representation. The paper first notes that LatPlan relies on (apparently accidentally) minimizing entropy in the discrete latent representation for its success. The paper then proposes a "zero-suppression" (which actually encourages *more* zeros in the latent representation) with the goal of encouraging a sparse representation that might be more resistent to "flipping" bits due to noise. Compared to the original LatPlan framework, the "zero-suppressed" version has lower variance in the latent states given noisy inputs, and solves more planning problems in the presence of noise.
 
# Review:
 
# First of all, "zero-suppressed" suggests the opposite of what the proposed method actually does. "Zero-enhanced" or "sparse" or "L0-regularized" would all be better names. I'll call the method "ZSAE" in the remainder of the review.
 
# The ZSAE method is a minor extension of the earlier LatPlan framework. The experimental results suggest that this extension achieves its objective of making the learned discrete representation more stable in the presence of noise, with a corresponding benefit to planning success. The observation that a *low entropy* objective for the latent representation makes it more stable is quite interesting and may be useful for other applications of VAEs with discrete latent variables.
 
# The paper's main weakness is an overall lack of clarity and completeness. I was able to get a general understanding of the modified LatPlan framework from the paper, but there are many important details missing. The most important missing pieces relate to how action models are created and how planning performance is actually evaluated. The two "AMA" methods are hardly described at all.
# I gather from reading the LatPlan paper that AMA1 exhaustively examines all possible (s, a, s') transitions for the true actions $a$ and learned state representations $s,s'$.
# So in this case the planner has access to the true actions and we can verify whether the computed plan actually succeeds in the real world.

# In the AMA2 method, though, the system is *learning* the action space as the latent space of an autoencoder that reconstructs successor states. The planner can plan in this entirely-learned space, but how do we know which real action a learned action corresponds to, so that we know what the planner actually wants to do in a given state and what the real reaults of that action are?

> I gather from reading the LatPlan paper

There is no need to read the previous work as long as understanding that the problem is a simple graph search.
We described AMA1 in sec.6.3:

> idealistic AMA that ... generates the entire propositional state transitions from the entire image transitions

The resulting problem to be solved is nothing more than a simple pathfinding over a graph defined by nodes and edges,
thus it is straightforward to convert it into a PDDL without further explanation.
In fact, we do not have to use a PDDL solver (other than presentation/demonstration purpose) --- in-memory graph search would do.

> all possible (s, a, s') transitions for the true actions $a$

There are no action labels assigned to the state pairs by humans.
Thus when converting the state space into a PDDL, each (s, s') pair becomes a single zero-ary (grounded) pddl action.
N edges become N action schema.

> has access to the true actions and we can verify whether the computed plan actually succeeds

There are no "true actions", e.g. human-understandable labels such as "move-up", "move-down".
We verify the results by looking at the visualized state transitions extracted by the decoder.
(In the experiments, we used Latplan's domain-specific validator which works on the output images.)

> In the AMA2 ... how do we know which real action a learned action corresponds to

We don't. The input lacks supervised labels for actions, and AAE in AMA2 finds its own set of actions by clustering the transitions (e.g. "this transition and this transition look similar, thus should belong to the same action")
We verify the final visualized results only.


* R3

# Significance: 	
# 1: (minimal contribution or weak impact)
# Soundness: 	
# 3: (correct)
# Scholarship: 	
# 1: (important related work missing, or mischaracterizes prior research)
# Clarity: 	
# 2: (mostly readable with some room for improvement)
# Reproducibility: 	
# 3: (authors describe the implementation and domains in sufficient detail)
# Overall evaluation: 	
# -1: (weak reject)

# This paper presents an improvement on existing image-based planning
# leveraging classical planners. The idea is to first learn the set of state
# variables (propositions), then learn an action model, followed by classical
# planning. The drawback of the standard approach as well as previous work
# (State AutoEncoder) is the high stochasticity, which the authors call the
# stability problem of the learned propositional encoding.

# It is notable that the authors found a bug in the implementation of the primary
# previous work SAE that differed from the paper, that helped LatPlan work better
# than expected. Besides this, the insights and proposed algorithm here are
# incremental and the results not surprising, not substantial enough for an ICAPS
# paper.
# 
> The paper does not discuss ... Sparse AutoEncoder ... impose a sparsity constraint on the discrete latent space

We discussed (Sec5,bottom,left) Sparse AE, which is a synonym for a l1-regularized AE with *continuous* activations (Sec14.2.1,Sparse Autoencoders,Goodfellow et.al,2016).
Discrete representation learning with NN is a fairly new topic which made the first success in (Jang et.al,ICLR17).
To our knowledge, there is no regularization proposed for such discrete representation.
Assymmetric treatement of categorical activations is clearly different from the standard l1-regularization.

# Definition 1 and Definition 2 seem to be loosely stated "under some equivalence
# relation". Further, it seems the definitions are not used elsewhere in the
# paper?

> "under some equivalence relation"

In the particular cases in this paper, this is the threshold on the error noticeable by humans.

Other instances include a symbolic representation "A is adjascent to B" of a picture depicting A and B,
which is rotation and translation invariant to the absolute coordinates of A and B.

# One trick used in VAEs is to turn off the stochasticity in the input->latent
# mapping --- simply take the mean or most likely outcome of the distribution.
# Would this satisfy the stability criterion? This needs to be shown as the basic
# remedy to the stochasticity/stability problem.

Even if the entire network becomes determinsitic with the argmax trick,
the external purturbation in the input (image) alters the latent representation.
We can see this from both Table 2 ---
AMA1-based planner with argmax can successfully solve all problems with the clean inputs, while it fails in multiple instances with the noisy input.
Therefore, argmax is not sufficient for addressing the external stochasticity and zero-suppression is necessary for the noisy inputs.
 
# Similarly, in GS-VAE as temperature goes to zero, the stochasticity in the
# latent encodings should also tend to deterministic. It seems the stability
# problem stems more from stochasticity in the input rather than encodings. The
# issue with small variations in input leading to large deviations in NN outputs
# is well known, and perhaps a look at these adversarial examples might shed some
# light in to the symbol stability problem.
# 
# At a more fundamental level, stable symbols are not as import as predictive
# symbols that learn meaningful action models, beyond reconstruction of the
# current image. It would be interesting if the authors expand the discussion
# around the different design choices for symbolic learning.
# 
# I really like the flavor of experiments and the domains used. However, it is
# hard to judge the differences based on the total sample variance alone.
# They do not show the reconstruction error or any generated samples.
# The authors show planning performance in Table 3, but it could be expanded.

* local variables                                                  :noexport:

# Local Variables:
# truncate-lines: nil
# eval: (load-file "publish-and-count-word.el")
# End:

