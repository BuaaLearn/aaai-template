#+TITLE: 
#+DATE: 
#+AUTHOR: 
#+EMAIL: 
#+OPTIONS: ':nil *:t -:t ::t <:t H:3 \n:nil ^:t arch:headline author:nil
#+OPTIONS: c:nil creator:nil d:(not "LOGBOOK") date:nil e:t email:nil
#+OPTIONS: f:t inline:t num:t p:nil pri:nil stat:t tags:t tasks:t tex:t
#+OPTIONS: timestamp:nil toc:nil todo:t |:t
#+CREATOR: Emacs 24.3.1 (Org mode 8.2.10)
#+DESCRIPTION:
#+EXCLUDE_TAGS: noexport
#+KEYWORDS:
#+LANGUAGE: en
#+SELECT_TAGS: export


Thank you for the thoughtful reviews.
All minor issues reported in the reviews will be revised.
In the following, we abbreviate Reviewer #X to RX.
Individual responses come after the overall response.

Re: Reproducibility.
ZSAE code is heavily based on the original Latplan. We are planning to publish the source code similarly.
The dataset is equivalent to the one in Latplan.


Re: Details of Latplan.

We described all the important details that are directly related to this paper's contribution(ZSAE), which is NNs, GumbelSoftmaxVAE, SAE, training, and its probabilistic model.

While we fully understand the inner workings of Latplan, we described its AMA/planning/validation part as a black box in this paper, just as treating Fast Downward/VAL as a black box planner/validator.
We did not modify the other parts of Latplan and Latplan itself is not our contribution.

Still, we described AMA1 and AMA2 in Sec.6.3.
AMA1 directly encodes the state space graph into a PDDL, with all state transitions provided (oracular model).
AMA2 has two networks, (1) AAE is a modified autoencoder model with GumbelSoftmax that learns action labels unsupervised, and (2) AD is a binary classifier for learning action preconditions.
Also note that we provided further elaborated details of AMA1/AMA2 in the section 5 of the supplement.
This is an almost verbatim copy from Latplan paper with authors' permission.
We further discuss this in reply to R2.

* R1

# Significance: 2: (modest contribution or average impact)
# Soundness: 3: (correct)
# Scholarship: 2: (relevant literature cited but could be expanded)
# Clarity: 3: (well organized and well written)
# Reproducibility: 3: (authors describe the implementation and domains in sufficient detail)
# Overall evaluation: 2: (accept)
# Review:

# This paper extends an existing approach for learning symbolic state representations in planning domains (well.. that is what it is used for, but it coule be used for other things). The idea is to put more constraints on an auto-encoder network setup such that the latent space is forced into a more "stable" bit representation. The authors define what that means and contribute the algorithm, an analysis of the previous algorithm (using a novel viewpoint) and the introduction of the symbol stability problem. Many experiments are included to test various aspects and to compare to two previous approaches.
# 
# This paper is well-written, focused and it contains insightful experiments for what the authors claim to contribute. It is interesting to see that in addition to a new algorithm, the authors also analyze the original algorithm and find out (confirmed by contact with the original authors) that even the original algorithm was different from its description.
# 
# This paper is about an important problem: with all the deep learning success, it is good to look at how such models can be used to obtain representations that are useful for (symbolic) planning, and especially how we can obtain stable representations. The problem setting is very clear from the start, all the sub-steps and problems are well introduced and also covered in the experiments, and terminology is clear throughout the paper. Most of the questions I had while reading were answered right away or through the experiments. The first half of the paper could use a more extensive example to get hands-on with the problem of stability; I agree that the pictures do introduce it, but on a slightly more abstract level though. Some of the language can be improved (some small things like literals missing, but overall the paper is quite polished already). Figure 4 is not very clear (compared to the rest of the paper).
# 
# Section 3 might overdo it a little when explaining things related to the main theme of the paper; I guess some of it is redundant.

# I think that all experiments "before" the actual planning tests are insightful and convincing (also the comparisons).

# For the planning experiments themselves, I think these are not overly convincing.
> I do see the effects of the new regularization on the latent representation, and the effect on planning,
> but these are not too large

> The number of solved instances is almost the same

**The main scores (table 3) have more than tripled with AMA1 (14-43,6-48,6-33) and more than doubled with AMA2 (29->81,24->62).**

We discussed that the source of stochasticity is either internal or external(Sec.3,paragraph.1),
that argmax supresses the internal one(Sec.4,last par),
and that the ZSAE regularization suppresses the external one(Sec.6.1). 

**The main evaluation result of ZSAE is therefore the evaluation with the noisy input**, NOT the clean input which lacks external stochasticity -- We will clarify this in the revision.
The gaussian noise (Ïƒ=0.6 for pixel values between 0.0 and 1.0) is so strong that we hardly recognize the original content (e.g. mandrill) in the image (see [anonymous image link]),
but the system is able to map it to the appropriate propositional state and solves it successfully.

Indeed, while the ZSAE may have only a small effect when the input is clean,
it does not reduce its impact at all because virtually all real-world data contain various forms of noise/perturbation, and obtaining the clean input is extremely rare.

Also, note that N=36 for SAE is a result of hyperparameter search, thus comparing this best case with a ZSAE with different N is unfair.

# The number of solved instances is almost the same, but according to the end of section 6.3. search efforts and runtimes do differ, but I think more experiments/analysis is needed here.
# This is the only weaker point of the paper, since it is the main focus (seeing how better representations enable "better" planning).
# I also feel that if one leaves the planning domain aside, the experimental section could have appealed to other methods too that work on compression of (auto-encoder based) learning.
# The related work could also be expanded somewhat if looking more in this direction.
# 
# Nevertheless, this is a nice paper with interesting results.

* R2

# Significance: 1: (minimal contribution or weak impact) Minor extension of an already-published method
# Soundness: 2: (minor inconsistencies or small fixable errors)
# Scholarship: 2: (relevant literature cited but could be expanded)
# Clarity: 2: (mostly readable with some room for improvement)
# 
# Many important details are not described precisely. Understanding the system requires reading the earlier LatPlan paper (Asai and Fukunaga, 2018), which is itself difficult to parse.
# 
# Reproducibility: 	
# 3: (authors describe the implementation and domains in sufficient detail)
# Would be very difficult to reproduce from this paper alone, but the work is an extension of the LatPlan system, which has available source code.

# # shared comments
 
# Overall evaluation: 	
# -1: (weak reject)
# 
# Review: 	Summary:

# The paper proposes an extension to the LatPlan system (Asai and Fukunaga, 2018) to improve the "stability" of the learned discrete state representation. The paper first notes that LatPlan relies on (apparently accidentally) minimizing entropy in the discrete latent representation for its success. The paper then proposes a "zero-suppression" (which actually encourages *more* zeros in the latent representation) with the goal of encouraging a sparse representation that might be more resistent to "flipping" bits due to noise. Compared to the original LatPlan framework, the "zero-suppressed" version has lower variance in the latent states given noisy inputs, and solves more planning problems in the presence of noise.
 
# Review:
 
# First of all, "zero-suppressed" suggests the opposite of what the proposed method actually does. "Zero-enhanced" or "sparse" or "L0-regularized" would all be better names. I'll call the method "ZSAE" in the remainder of the review.

> "zero-suppressed"

The name comes from Zero-Suppressed Decision Diagram [Minato ACM93], an established method which prunes DD nodes that points to constant 0 node. ZSAE also allows to prune constant 0 neurons.
Minato gave an invited talk in SoCS+ICAPS in 2017.

# The ZSAE method is a minor extension of the earlier LatPlan framework. The experimental results suggest that this extension achieves its objective of making the learned discrete representation more stable in the presence of noise, with a corresponding benefit to planning success. The observation that a *low entropy* objective for the latent representation makes it more stable is quite interesting and may be useful for other applications of VAEs with discrete latent variables.
 
# The paper's main weakness is an overall lack of clarity and completeness. I was able to get a general understanding of the modified LatPlan framework from the paper, but there are many important details missing. The most important missing pieces relate to how action models are created and how planning performance is actually evaluated. The two "AMA" methods are hardly described at all.
# I gather from reading the LatPlan paper that AMA1 exhaustively examines all possible (s, a, s') transitions for the true actions $a$ and learned state representations $s,s'$.
# So in this case the planner has access to the true actions and we can verify whether the computed plan actually succeeds in the real world.

# In the AMA2 method, though, the system is *learning* the action space as the latent space of an autoencoder that reconstructs successor states. The planner can plan in this entirely-learned space, but how do we know which real action a learned action corresponds to, so that we know what the planner actually wants to do in a given state and what the real reaults of that action are?

# > I gather from reading the LatPlan paper
# 
# There is no need to read the previous work as long as understanding that the problem is a simple graph search.

> completeness

We did not modify any AMA/planning/validation modules of Latplan and thus treating them as a black box.

We still described AMA1 in sec.6.3:

  idealistic AMA that ... generates the entire propositional state transitions from the entire image transitions

The resulting problem to be solved is nothing more than a simple pathfinding over a graph defined by nodes and edges,
thus the conversion into PDDL is trivial.
In fact, we do not have to use a PDDL solver (other than presentation/demonstration purpose) --- in-memory graph search would do.

> ...all possible (s, a, s') transitions for the true actions $a$
> ...has access to the true actions and we can verify whether the computed plan actually succeeds
> In the AMA2 ... how do we know which real action a learned action corresponds to

As noted in (Sec.2, input specification), there are no ground-truth action labels assigned to the state pairs by humans, such as "move-up", "move-down".
Thus when converting the state space into a PDDL, each (s, s') pair becomes a single zero-ary (grounded) action. N edges become N action schema.

We also described AMA2 in sec.6.3. This is much more detailed compared to AMA1.
AMA2 finds its own set of actions by clustering the unlabelled transitions (page7,left,paragraph.2) (e.g. "this transition and this transition look similar, thus they should belong to the same action schema, whatever it is"),
Since the action labels are made up by the NN itself and their meaning unknown, there is no way to verify the plan from the action labels.

Latplan verifies the results by looking at the visualized state transitions extracted by the decoder.
In the experiments, we used Latplan's domain-specific validator which works on the output images.
These are all included in the black-box evaluation suite that comes with Latplan's source code.


* R3

# Significance: 	
# 1: (minimal contribution or weak impact)
# Soundness: 	
# 3: (correct)
# Scholarship: 	
# 1: (important related work missing, or mischaracterizes prior research)
# Clarity: 	
# 2: (mostly readable with some room for improvement)
# Reproducibility: 	
# 3: (authors describe the implementation and domains in sufficient detail)
# Overall evaluation: 	
# -1: (weak reject)

# This paper presents an improvement on existing image-based planning
# leveraging classical planners. The idea is to first learn the set of state
# variables (propositions), then learn an action model, followed by classical
# planning. The drawback of the standard approach as well as previous work
# (State AutoEncoder) is the high stochasticity, which the authors call the
# stability problem of the learned propositional encoding.

# It is notable that the authors found a bug in the implementation of the primary
# previous work SAE that differed from the paper, that helped LatPlan work better
# than expected. Besides this, the insights and proposed algorithm here are
# incremental and the results not surprising, not substantial enough for an ICAPS
# paper.
# 
> The paper does not discuss... Sparse AutoEncoder... impose a sparsity constraint on the discrete latent space

We discussed (Sec5,bottom,left) Sparse AE, which is a synonym for a l1-regularized AE with *continuous* activations (Sec14.2.1,Sparse Autoencoders,Goodfellow et.al,2016,http://deeplearningbook.org/).
Discrete representation learning with NN is a fairly new topic which made the first success in (Jang et.al,ICLR17).
To our knowledge, there is no regularization proposed for such discrete representation.
Assymmetric treatement of categorical activations is clearly different from the standard l1-regularization.

Despite that, we did not try to claim its novelty as a general machine learning method, as we focused on the planning aspect and symbol stability.

# Definition 1 and Definition 2 seem to be loosely stated "under some equivalence
# relation". Further, it seems the definitions are not used elsewhere in the
# paper?

> "under some equivalence relation"

In the particular cases in this paper, this is the equivalence under the error threshold noticeable by humans.

Other instances include a symbolic representation "A is adjascent to B" of a picture depicting A and B,
which is rotation and translation invariant to the absolute coordinates of A and B in the picture.

# One trick used in VAEs is to turn off the stochasticity in the input->latent
# mapping --- simply take the mean or most likely outcome of the distribution.
# Would this satisfy the stability criterion? This needs to be shown as the basic
# remedy to the stochasticity/stability problem.

> turn off the stochasticity...Would this satisfy the stability criterion?

Argmax trick does not fully satisfy the stability criterion.
Even with the trick,
the external purturbation in the input (image) alters the latent representation.
We can see this from both Table 2:
AMA1-based planner with argmax can successfully solve all problems with the clean inputs, while it fails in multiple instances with the noisy input.
Therefore, argmax is not sufficient for addressing the external stochasticity and zero-suppression is necessary for the noisy inputs.
 
> as temperature goes to zero, the stochasticity...tend to deterministic...
> ...the stability problem stems more from stochasticity in the input

Hard to say which effect is larger (internal/external stochasticity).

# Similarly, in GS-VAE as temperature goes to zero, the stochasticity in the
# latent encodings should also tend to deterministic. It seems the stability
# problem stems more from stochasticity in the input rather than encodings. The
# issue with small variations in input leading to large deviations in NN outputs
# is well known, and perhaps a look at these adversarial examples might shed some
# light in to the symbol stability problem.
# 
# At a more fundamental level, stable symbols are not as import as predictive
# symbols that learn meaningful action models, beyond reconstruction of the
# current image. It would be interesting if the authors expand the discussion
# around the different design choices for symbolic learning.

# don' answer, not clear what he means

# I really like the flavor of experiments and the domains used. However, it is
# hard to judge the differences based on the total sample variance alone.
# They do not show the reconstruction error or any generated samples.
# The authors show planning performance in Table 3, but it could be expanded.

> They do not show the reconstruction error or any generated samples.

Table.1 shows MSE for the test dataset and we spent entire 6.2 for discussing the accuracy.
We did not include the visualizations as we already showed the absolute numbers.

* local variables                                                  :noexport:

# Local Variables:
# truncate-lines: nil
# eval: (load-file "publish-and-count-word.el")
# End:

