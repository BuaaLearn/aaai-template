#+TITLE: 
#+DATE: 
#+AUTHOR: 
#+EMAIL: 
#+OPTIONS: ':nil *:t -:t ::t <:t H:3 \n:nil ^:t arch:headline author:nil
#+OPTIONS: c:nil creator:nil d:(not "LOGBOOK") date:nil e:t email:nil
#+OPTIONS: f:t inline:t num:t p:nil pri:nil stat:t tags:t tasks:t tex:nil
#+OPTIONS: timestamp:nil toc:nil todo:t |:t
#+CREATOR: Emacs 24.3.1 (Org mode 8.2.10)
#+DESCRIPTION:
#+EXCLUDE_TAGS: noexport
#+KEYWORDS:
#+LANGUAGE: en
#+SELECT_TAGS: export



Thank you for the thoughtful reviews.
All minor issues reported in the reviews will be revised.
In the following we abbreviate Reviewer #X to RX.
Individual responses come after the overall response.

As we stated in the Conclusion, the contributions of this paper is threefold:
 1. Rigorous formal analysis of the neural network model used in Latplan system.
 2. ZSAE.
 3. Introduction of SSP. (meta-level)
Thus section 4 corresponds to the first contribution.

The original Latplan paper did not provide this kind of theoretical analysis of the
system as we performed in this paper.

# Besides, both authors of the original Latplan paper do not have a deep expertize in
# machine learning (personal communication with the authors), thus we hope to
This provides a useful basis that the symbolic planning community can stand upon
for extending this line of work.


In the paper, we pointed to the supplement as "details" several times.
Sometimes the "details" are more like extended footnotes.
In fact, the section S2.3 in the supplement is merely a single paragraph of 3 lines.

Importantly, these are attempts to help the readers familiarize themselves
to the terms and ideas in the machine learning / neural network literature, as
we expected readers from both the symbolic and the machine-learning sides.
While this paper requires the understanding of both fields,
space limitation does not allow us to include every details of machine-learning in the paper.
If machine-learning researchers review this paper, they would also
have slight difficulty understanding the underlying assumptions of classical planning.

This paper is self-contained. We provide enough information
to reproduce the implementation and experiments. Codes will also be released
after acceptance.

To R3:

: Section 4 ... not clear what this section is adding to the paper.

: In summary, ... this paper seems to make an incremental step which is not fully explained in
: the paper (see section 5), and which does not appear significant enough for a AAAI.

Section 4 provides a rigorous formal analysis of the neural network model used in
Latplan system, which was not provided in the original Latplan paper.

Also, *TODO*

: Most importantly, the heart of the paper - details of the ZSAE method - are
: apparently only available in the supplemental material.

: I could not find sufficient details of ZSAE - are they is the paper?

This paper is self-contained.

The network architecture of ZSAE is identical to that of SAE, as we mentioned:
"Zero-Suppressed State AutoEncoder (ZSAE), a SAE with an additional regularization".
The only difference between them is the additional term in the optimization metric,
which is shown in the formula in section 5.

To reiterate, it "penalize the true propositions in the latent layer"
by penalizing \sum_n z_n1 
for the latent layer z_nk (an element of a matrix), k ∈ {0, 1},
where the matrix is constrained to z_n0 +z_n1 = 1
(this is one of the basic characteristics of gumbel-softmax).

We stated the binary variable b_n (used in the mathematical model in section 4)
is equivalent to z_n1.
If z_n1 = 1 (therefore z_n0 = 0), it means that the n-th proposition is true.
Therefore the term (\sum_n z_n1) is explicitly penalizing true propositions.
We thus provided sufficient information to reproduce the implementation.

### it would not be useful to refute "though not explicitly stated" part
# : The evaluation I assume (though not explicitly stated) is to show that the
# : ZSAE is superior to the SAE.
# 
# In the abstract as well as in the introduction/conclusion,
# we clearly stated that ZSAE improves upon SAE.
# 
# + Abstract:     "“Zero-Suppressed SAE”, an enhancement..."
# + Introduction: "ZSAE obtains a more "stable" propositions..."
# + Conclusion:   "...which improves the vanilla SAE".


: Please explain the Aims/Objectives of the Empirical Evaluation and the reason
: for the metrics used.


The aim of sec6.1 is twofold:
The first one is to show the stability of the obtained propositions as measured by the
state variance for the same / almost same (purturbated by noise) input image.
We obtained the propositional vectors of the same / almost same image using the SAE/ZSAE.
If the variance of the propositional vector is high, it means
that the network tends to return different state encoding even for the same image observation,
which is harmful for planning.
Thus, the ZSAE, which achieves the lower variance, is superior. This is the main claim.

The second aim is to understand the other overall characteristics of the ZSAE as
compared to SAE in order to provide further insights into some concerns and why
it is successful (e.g. the effect of zero-suppression penalty on the output
accuracy, effective bits).

The third aim is to show the additional benefits of using ZSAE (e.g. ZSAE is
less sensitive to hyperparameters, thus a ZSAE is easier to train than a SAE).


# # maybe describing 6.2 and 6.3 is not necessary.
# # Apparently none of the reviewers are concerned with 6.3, so let's not
# # wake a sleeping dragon.
# # Reviewer 3 only mentions the variance metrics.
# The aim of sec6.2 is to show the success rate of classical planning in the
# propositional state space is higher when they are produced by ZSAE rather than
# SAE.  Also, we addressed the impact of the unstable representation (e.g. graph
# disconnectedness and duplicate detection in section 3) are reduced by
# using ZSAE.
# 
# The aim of sec6.3 is a simple demonstration that ZSAE allows 

To R1:

: Due to some design decisions of Latplan and how NNs work, the resulting
: propositional representations could have problems related to stability

I think his confidence is a bit lower

: the representations generated in two time steps could differ due to some
: stochasticity in the learning procedure

"two time steps" -> unsure about what he implies, it is for single time step
Also, stochasticity prevails after the learning procedure too


: As a detailed comment, you should explain ARM_2 when it is first
: referenced in the Introduction.

yes

: When you describe Latplan in Sec 2, given that it does not get as
: input labels for actions, should we assume Latplan generates a
: completely instantiated domain?

not sure what s/he means by "instantiated domain"

yes for AMA1

AMA2 does not return a PDDL, but a black box neural action model.


: You assume b_n to be independent in Sec. 4. It is clear that it
: greatly simplifies the math. But, does it have any implication in the
: results? As far as I understand your work in terms of planning,
: propositions are not usually (or necessarily) independent.

(not sure)


To R2:

: the base paper (Asai and Fukunaga 2018) tackles this issue already in a
: first way and now the authors suggest an additional regularization.

The base paper did not explain the Entropy Regularization, a diversion from
the regular Gumbel-Softmax VAE.
Thus, the base paper did not address the stability issue, only their implementation did.

: In table 1, middle
: column about MSE, the authors speak about the orders of magnitude larger MSE for
: N=36, but for N=100/1000 the same happens in the LightsOut domain (which
: interestingly was not problematic for N=36). Do you have an explanation
: for this?

Typically, MSE below 1.0e-3 is visually not significant to human (both 2.8e-14, 1.2e-5).

: In the original SAE the Kullback-Leibler divergence helps stabilizing the latent
: representation of the state. Your ZSAE uses both the KL divergence and your own
: regularization. Have you tested/Can you test what the effect of your own
: regularization alone is?

no... (should we start this experiment)

: The way you have written down your own regularization allows the latent
: representation to be non binary. What are your thoughts about using non binary
: predicates (like in SAS+ representation a variable can have multiple values)?

It was intentionally written so with SAS+ in mind
(e.g. if k \in {0,1,2}, it is a variable with 3 values).
The base paper (Asai 2018) also mentions SAS+.
The expressivity of the representation is not affected by limiting the domain to
binary values (as STRIPS and SAS+ is equivalent).
However, hand-coding the number of possible values for each variable
would require human effort.

: In Section 6.1 your have written that because of the probabilistic nature of the
: latent representation you encoded the same image 100 times and took the mean. In
: my understanding the mean would lead you to have continous values whereas the
: system later operates on 0 and 1.

We encoded the same image 100 times and took the *variance*.
Then, for the 100 *different* images, we took the mean of the variance.

Also, the value is just a metric for measuring the stability of the NN in this experiment.
this value is not used in the later NN pipeline and the system.

: (This would also be a question in LatPlan) In a real world setting without a
: ground truth to check for, do you have an idea how to select N correctly?

After the training, the correctness of the SAE is checked by
applying the SAE to an unseen set of images (test instances)
and checking the error between the input and the reconstrcution.
Since the input (raw observation) is the ground truth by itself,
we can tell that the NN is not learning if the error is large.

With the vanilla SAE, you have to rely on try-and-errors to find the best N.
If N is too small, the network cannot represent the world.
Too large N also causes the stability issues.

With ZSAE, we can set N very large and let the zero-suppression reduce the
number of effective bits automatically.

In practice, the size of N would be restricted by the hardware limitation (GPU)
and the time constraint (large network = slow training).

* local variables                                                  :noexport:

# Local Variables:
# truncate-lines: nil
# eval: (load-file "publish-and-count-word.el")
# End:

