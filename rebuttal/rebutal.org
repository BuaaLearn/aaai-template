#+TITLE: 
#+DATE: 
#+AUTHOR: 
#+EMAIL: 
#+OPTIONS: ':nil *:t -:t ::t <:t H:3 \n:nil ^:t arch:headline author:nil
#+OPTIONS: c:nil creator:nil d:(not "LOGBOOK") date:nil e:t email:nil
#+OPTIONS: f:t inline:t num:t p:nil pri:nil stat:t tags:t tasks:t tex:t
#+OPTIONS: timestamp:nil toc:nil todo:t |:t
#+CREATOR: Emacs 24.3.1 (Org mode 8.2.10)
#+DESCRIPTION:
#+EXCLUDE_TAGS: noexport
#+KEYWORDS:
#+LANGUAGE: en
#+SELECT_TAGS: export



Thank you for the thoughtful reviews.
All minor issues reported in the reviews will be revised.
In the following we abbreviate Reviewer #X to RX.
Individual responses come after the overall response.

As we stated in the Conclusion, the contributions of this paper is threefold:
 1. Rigorous formal analysis of the neural network model used in Latplan.
 2. ZSAE.
 3. Introduction of SSP. (meta-level)
Thus, section 4 corresponds to the first contribution.

(Asai 2018) did not provide this kind of theoretical analysis as we performed in this paper.
Thus we provide a formal basis that the planning community can use for extending this line of work.

(Asai 2018) neither mentioned that their loss function was inverted from
the standard Gumbel-Softmax. Therefore, while their implementation accidentally
"tackles this issue already", it was not noticed and not published in any material.

Finally, finding of "Entropy Regularization"
showed that "Regularization on latent variables is the key to solve SSP",
which naturally lead to a new regularization, Zero-suppression.
We will emphasize this point in the revision.

# Besides, both authors of the original Latplan paper do not have a deep expertize in
# machine learning (personal communication with the authors), thus we hope to


Next, this paper is self-contained. We included enough information in the paper
to reproduce the implementation from the Latplan code available from github.
Our code will also be released after acceptance.

We pointed to the supplement as "details" several times which 
might have given the impression that it is otherwise.
They are attempts to help the readers understand our math, map the math to the implementation, or familiarize themselves
to the ideas in the machine learning, because
we expected readers from both the symbolic and the machine-learning sides.
# While this paper requires the understanding of both fields,
# space limitation does not allow us to introduce basic concepts.
# If machine-learning researchers review this paper, they would also
# have slight difficulty understanding the underlying assumptions of classical planning.
Thus sometimes the "details" are just extended footnotes.
Indeed, the supplemental Sec. S2.3 linked above
the ZSAE formula (p.5) contains merely a paragraph of 3 lines.

It should be clear that the "details" are meant to bridge the gap between two communities,
i.e. machine learning and classical planning.

# In deep learning, it is usual to just give the math and proceed to experiments, expecting the reader to implement the algorithm.

To R3:

# : Section 4 ... not clear what this section is adding to the paper.
# 
# : In summary, ... this paper seems to make an incremental step which is not fully explained in
# : the paper (see section 5), and which does not appear significant enough for a AAAI.
# 
# Section 4 provides a rigorous formal analysis of the neural network model used in
# Latplan system, which was not provided in (Asai 2018).

# Most importantly, the heart of the paper - details of the ZSAE method - are
# apparently only available in the supplemental material.
# The evaluation I assume (though not explicitly stated) is to show that the
# ZSAE is superior to the SAE. 6.1 attempts this using the 'variance' as a
# metric. This seems logical, but without enugh details of the ZSAE method in the
# paper, it is difficult to judge.

: details of the ZSAE method ...

# : I could not find sufficient details of ZSAE - are they is the paper?

# This paper is self-contained.

The network architecture of ZSAE is identical to that of SAE:
"Zero-Suppressed State AutoEncoder (ZSAE), a SAE with an additional regularization" (p.4-5).
The difference is the additional term in the optimization metric fully specified in section 5.

We stated that
the binary variable b_n (used in the formal model in section 4) corresponds to z_n1,
for the latent layer z_nk (a matrix element), k ∈ {0, 1},
where the matrix is constrained to z_n0 +z_n1 = 1
(basic characteristics of gumbel-softmax).
If z_n1 = 1 (therefore z_n0 = 0), it means that the n-th proposition is true.
Thus penalizing Σ_n z_n1 is to "penalize the true propositions in the latent layer".
This provides sufficient information to reproduce the implementation,
but in the revision, we will elaborate more on this.


# ## it would not be useful to refute "though not explicitly stated" part
# : The evaluation I assume (though not explicitly stated) is to show that the
# : ZSAE is superior to the SAE.
# 
# In the abstract as well as in the introduction/conclusion,
# we clearly stated that ZSAE improves upon SAE.
# 
# + Abstract:     "“Zero-Suppressed SAE”, an enhancement..."
# + Introduction: "ZSAE obtains a more "stable" propositions..."
# + Conclusion:   "...which improves the vanilla SAE".


: explain the Aims .. the metrics ..

The aim of sec6.1 is threefold:

We showed that ZSAE obtains the more stable propositions than the Vanilla SAE does.
The stability is measured by the
variance of the propositional values obtained from the same / almost same (noisy) input image using Z/SAE.
# 
If the variance is high, it means
that the network tends to return different encoding for the same image, which is harmful for planning.
ZSAE achieves the lower variance, thus it is superior.

Second, we compared the other overall characteristics between ZSAE and SAE
in order to provide further information and insights into its success
(e.g. the effect of zero-suppression on the output accuracy, effective bits).

Third, we showed the additional benefits of using ZSAE (e.g. 
less sensitive to hyperparameters, thus easier to train than a SAE).


# # maybe describing 6.2 and 6.3 is not necessary.
# # Apparently none of the reviewers are concerned with 6.3, so let's not
# # wake a sleeping dragon.
# # Reviewer 3 only mentions the variance metrics.
# The aim of sec6.2 is to show the success rate of classical planning in the
# propositional state space is higher when they are produced by ZSAE rather than
# SAE.  Also, we addressed the impact of the unstable representation (e.g. graph
# disconnectedness and duplicate detection in section 3) are reduced by
# using ZSAE.
# 
# The aim of sec6.3 is a simple demonstration that ZSAE allows 

To R1:

# : Due to some design decisions of Latplan and how NNs work, the resulting
# : propositional representations could have problems related to stability
# 
# I think his confidence is a bit lower

# : the representations generated in two time steps could differ due to some
# : stochasticity in the learning procedure
# 
# "two time steps" -> unsure about what he implies, it is for single time step
# Also, stochasticity prevails after the learning procedure too

# : As a detailed comment, you should explain ARM_2 when it is first
# : referenced in the Introduction.
# 
# yes

# : When you describe Latplan in Sec 2, given that it does not get as
# : input labels for actions, should we assume Latplan generates a
# : completely instantiated domain?
# 
# not sure what s/he means by "instantiated domain"

: ... Latplan generates a completely instantiated domain?

Both AMA1 and AMA2 returns a search space equivalent to a grounded STRIPS problem.

AMA1 returns a PDDL model where each (:action...) corresponds to each edge in the state space graph.

AMA2 takes an upper bound of the possible number of action labels (e.g. 128).
State transitions do not contain action labels, but AMA2 automatically groups similar transitions together
and assigns the same label, i.e., finds the action schema by itself.
AMA2 does not return a PDDL, but a neural network as a black box successor function succ(a,s)
that takes an action number (e.g. a ∈ [1..128]) and the current state.

# : You assume b_n to be independent in Sec. 4. It is clear that it
# : greatly simplifies the math. But, does it have any implication in the
# : results? As far as I understand your work in terms of planning,
# : propositions are not usually (or necessarily) independent.
# 
# (not sure)

# モデル上は、潜在表現bを得るときに各bit独立にガンベル分布からサンプルしてるから独立（ここ以外ランダム性ないですよね？）だけど、最終的に得られる潜在表現はほぼ deterministic だからこの仮定はあんまり本質的ではない、とか

: b_n to be independent .. implication .. ?

This means each b_n is sampled from independent Gumbel-Softmax,
thus it is not essential.


To R2:

# # already answered
# : the base paper (Asai and Fukunaga 2018) tackles this issue already in a
# : first way and now the authors suggest an additional regularization.
# 
# The base paper did not explain the Entropy Regularization, a diversion from
# the regular Gumbel-Softmax VAE.
# Thus, the base paper did not address the stability issue, only their implementation did.

# : In table 1, middle
# : column about MSE, the authors speak about the orders of magnitude larger MSE for
# : N=36, but for N=100/1000 the same happens in the LightsOut domain (which
# : interestingly was not problematic for N=36). Do you have an explanation
# : for this?

: MSE ... explanation for this?

MSE below 1.0e-3 is visually not significant to human (both 2.8e-14, 1.2e-5)
--- only noise-level difference. Therefore this is not problematic.

# : In the original SAE the Kullback-Leibler divergence helps stabilizing the latent
# : representation of the state. Your ZSAE uses both the KL divergence and your own
# : regularization. Have you tested/Can you test what the effect of your own
# : regularization alone is?

: Can you test ... your own regularization alone is?

# no... (should we start this experiment)
We will add the results in the revision if requested. (we are not allowed to present new results here)

# : The way you have written down your own regularization allows the latent
# : representation to be non binary. What are your thoughts about using non binary
# : predicates (like in SAS+ representation a variable can have multiple values)?

: your own regularization allows ... SAS+ representation ... ?

It was written with SAS+ in mind (e.g. if k ∈ {0,1,2} = a variable with 3 values)
as (Asai 2018) also mentions SAS+.
# The expressivity of the representation is not affected by limiting the domain to
# binary values (as STRIPS and SAS+ is equivalent).
# However, hand-coding the number of possible values for each variable
# would require human effort.

# : In Section 6.1 your have written that because of the probabilistic nature of the
# : latent representation you encoded the same image 100 times and took the mean. In
# : my understanding the mean would lead you to have continous values whereas the
# : system later operates on 0 and 1.
: took the mean ... have continous values ...

We encoded the same image 100 times and took the variance of the discrete values.
We then took the mean of this variance over N bits, over 100 different images.
The "mean" operation is purely for taking the statistics of the output;
This is not part of the network, and the network still outputs discrete values.

# : (This would also be a question in LatPlan) In a real world setting without a
# : ground truth to check for, do you have an idea how to select N correctly?

: ... how to select N correctly?

After the training, the correctness of the SAE is checked by
applying the SAE to an unseen set of images (test instances)
and checking the error between the input and the reconstrcution.
Since the input (raw observation) is the ground truth by itself,
we can tell that the NN is not learning if the error is large.

With the vanilla SAE, you have to rely on try-and-errors to find the best N (p.3, right, "Thirdly...").
With ZSAE, we can set N very large and let the zero-suppression reduce the number of effective bits automatically.
In practice, the size of N is restricted by the hardware (GPU) and runtime constraint (large N/network = slow training).

* local variables                                                  :noexport:

# Local Variables:
# truncate-lines: nil
# eval: (load-file "publish-and-count-word.el")
# End:

