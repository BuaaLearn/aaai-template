\begin{abstract}
Despite the recent improvements in admissible heuristic search techniques
in classical planning, it is known that the the exponential growth of
search plateau is unavoidable even under the optimistic assumption.
 % 
We extensively investigate various existing myth on tiebreaking
 strategies and propose simple yet effective methods for improving the
 search performance within plateau.
 % 
 % 
 They do not depend on any particular heuristic, nor
 on multi-heuristic portfolio.
 They work even if the heuristic
 function no longer provides useful information.
 Moreover, they do not even try to obtain any further information from
 the domain.
\end{abstract}



The contribution of this paper is to add a new variety of improvements
to the admissible search techniques.  Recent work by Helmert and Roger
\shortcite{helmert2008good} claims that, even with an optimistic
assumption of \emph{almost-perfect heuristics}, \astar still has an
exponentially large plateau. They conclude that the further performance
improvement requires the techniques orthogonal to the heuristic
function, such as symmetry breaking, domain reduction, factored planning
etc.  Our tiebreaking strategies falls into this category.

This paper is organized as follows: The next section describes the
background and preliminaries on the theoretical aspects of
\astar. Next, we describe the details of our diversified tie-breaking
method and its theoretical implications.  Then we provide an empirical
result of our algorithm compared to traditional \astar with various
heuristics. We finally conclude with a discussion on the future work.

\section{Backgrounds and Preliminary}
\label{sec-1}

\subsubparagraph{\astar and perfect heuristics}

\astar is a state of the art algorithm for finding an optimal path in the
search space represented as a graph. 
\astar always returns an optimal solution when the heuristic function $h$ is
admissible, i.e., when it never overestimate the true distance to the goal
$h^*$.
% , and the nodes are first ordered according to $f$
% 
Thus, the best possible admissible heuristic function is $h^*$ itself, which is
called \emph{perfect heuristics}. However, computing $h^*$ is PSPACE-Complete,
which is as difficult as solving the problem itself and is not
practical.

%It is known that with a perfect heuristics the planner do not have to
% conduct any search: There are no multiple possibilities that the
% planner should examine on each node.

\emph{Almost perfect} heuristic function $h_c$ is a class of similar
impractical, theoretical functions which is also PSPACE-Complete to
compute \cite{helmert2008good}.  It has a constant error $c$ from the
perfect heuristic $h^*$, i.e., $h_c=h^*-c$.  The important finding by
\citeauthor{helmert2008good} is that even with this intractable and
impractical heuristic function, the number of the nodes in the last
plateau of the search becomes exponentially large as the problem size
increases.  Using this fact, they showed that relying only on the
improvement of the heuristic functions is not fruitful in the near
future, and the researchers should seek the other,
orthorgonal improvement.

These intractable heuristics are of course very hard and expensive to
compute. However, even the practical, tractable heuristic functions
which can be computed in polynomial time, such as \lmcut and M\&S, are
also very computationally expensive. Compared to the other functions,
these functions dominate the search time over the other factors of
planning algorithms, such as node insertion and deletion.

\section{Tie Breaking Criteria in \astar}

% Compared to the other algorithms which
% guarantee optimality such as $IDA^*$ \cite{korf1985depth}, \astar requires a large amount of % memory to store the search nodes.

\astar stores the search nodes into two priority queues called an \emph{open list} and a \emph{closed list}, sorted based on $f$ value. While the implementation of the priority queue varies, it has another degree of freedom called tie-breaking, i.e. how to select the next node to open within the same priority.

In the current implementation of the \sota admissible planning systen Fast Downward (FD) \cite{Helmert2006}, the standard \astar search breaks ties based on $h$, meaning that if two nodes have the same $f$ value, the nodes with smaller $h$ will be selected, in other words favoring the nodes with larger $g$ values (since $f=g+h$). The intension behind this is that the $h$ values are just an \emph{estimate} to the goal, while $g$ values are the \emph{actual} distance from the initial state and more reliable.

Another thing we have to note is that in FD, when even the $h$ values are the same, then the second tiebreaking behavior is basically a FIFO order: The planner selects the nodes that is inserted earlier. 

However, in fact, these tiebreaking methods are not necessary when we are only concerned with  maintaining the optimality. These tiebreaking criteria are just the result of heuristic, ad-hoc selection by humans and have no theoretical background.
% In particular, we first observed that this FIFO order has not legitimate reason to support.

We therefore ran a preliminary experiments on various benchmark domains, comparing a simple FIFO-order, LIFO-order and Random order using 5 minutes runtime cutoff. 
All experiments below are conducted on a cluster with Xeon E5410@2.33GHz CPUs.
We observed that even such a slightest difference can change the performance significantly, depending on the domains.
\refig{single-eval} compares the number of node evaluation (computations of \lmcut)
by LIFO, FIFO and Random second tiebreaking.
According to the figure, LIFO has smaller number of evaluations than the others in Openstacks, but Random dominates the others in Miconic and Nomystery, indicating that there are no dominance relationship between these three.
\reftbl{single-coverage} shows the coverage result (number of problems solved) by each strategy.
Overall, LIFO strategy achieved a higher coverage.

Note that, although LIFO dominated the others, we consider this is just by a coincidence due to our selection of problems, time limit and domains. we \emph{are not trying to claim that any of LIFO or FIFO or Random order dominates the other}. However, there are noticeable performance difference cause by these different tiebreaking strategies.

\begin{figure}[htbp]
 \centering
 \relsize{-2}
 \includegraphics{tables/aaai16-evaluated-lmcut_ff-lmcut_r.pdf}
 \includegraphics{tables/opt11-evaluated-lmcut_ff-lmcut_lf.pdf}
 \includegraphics{tables/opt11-evaluated-lmcut_lf-lmcut_r.pdf}
\caption{Comparison of the number of node evaluations (computations of \lmcut) by FIFO and LIFO tie breaking, on IPC2011 optimal track instance. LIFO order dominates FIFO and Random order especially in openstacks instances, and the gap is more than one the order of 10.}
 \label{single-eval}
\end{figure}

\begin{table}[htbp]
 \centering
 \relsize{-2}
 \input{tables/1_1_vs_seedonly30-aaai16-prelim3-opt11-concat.tex}
 \caption{Preliminary experiments comparing the performance of FIFO, LIFO and Random second-level tiebreaking using Fast Downward. Each cell denotes the problem solved with 5 minutes runtime, 2GB memory limitation. \textbf{Boldface} denotes the case where it achieved the best result among configurations. LIFO tiebreaking was obtained by modifying the TieBreakingOpenList in FD. Both FIFO and LIFO use $[g+h,h]$ as a sorting criteria, where $h=$\lmcut. For Random tiebreaking, we implemented a so-called ``random heuristics'' $r$, which always returns a random value, then used it as the second tiebreaking, i.e., $[g+h,h,r]$. The seed is initialized to 1.}
 \label{single-coverage}
\end{table}

We also observed such differences occur especially in the problems which have the huge search plateau, i.e., the problems where the planner relies heavily on the tiebreaking criteria.  \refig{plateau-h} shows the size of the bucket in a priority queue when the solution is found, compared to the total amount of effort in the search. 
The nodes in a same bucket shares the same $[f,h]$ value, therefore it cannot be guided by the  heuristic functions within this bucket.
Some domains clearly exhibits the large amount of effort is spent on searching through the last plateau, and those domains are greatly affected by the difference in the second tiebreaking such as LIFO, FIFO or Random, according to the previous figures.

In \refig{plateau-h}, we also compared the similar statistics where the $h$-based tiebreaking is disabled. We observed that much higher amount of effort is spent on the plateau based on single-element tiebreaking vector $[f]$.

\begin{figure}[htbp]
 \centering
 \relsize{-2}
 \includegraphics{tables/aaai16-front-vs-evaluated.pdf}
 \caption{Comparison of the size of the search plateau compared to the total evaluation. Data were obtained by the result of standard FIFO tiebreaking on the standard benchmark instances. Both axes are logarithmic. Each dotted line represents 10x, 100x ... lines.  Openstacks,  clearly has the large plateaus.}
 \label{plateau-h}
\end{figure}

\begin{figure}[htbp]
 \centering
 \relsize{-2}
 \includegraphics{tables/aaai16-front-vs-evaluated.pdf}
 \caption{Comparison of the size of the search plateau compared to the total evaluation. Data were obtained by the result of running \astar on the standard benchmark instances, with FIFO but without the tiebreaking by $h$. Both axes are logarithmic. Each dotted line represents 10x, 100x ... lines.}
 \label{plateau-f}
\end{figure}


We can have several important observations from these results.  Firstly, in a plateau, \textbf{the heuristic functions are not used at all, nor the search is guided at all}. This observation holds even if we combine several nondominating heuristics for tie breaking e.g. \lmcut and M\&S.  It is still possible that a plateau is encountered, since their combination is not a perfect heuristics yet!

Secondly, such a plateau is known to be inevitable even if we have an almost perfect heuristics $h_c$, and it is impossible to improve upon $h_c$ --- if it could, the result would be a perfect heuristics or an inadmissible heuristics. Therefore, this problem \textbf{cannot be solved by improving the heuristic accuracy, which is the currently dominating meta-strategy to improve the planner performance.}  Note that combining multiple heuristic functions by taking their maximum is still an attempt to improve the accuracy, therefore it does not solve this problem.

Thirdly, there is no legitimate reason which supports each tiebreaking strategy. \textbf{$h$ and FIFO are just heuristically chosen by the implementer of the planner.} Nor are there any reason to choose LIFO, or Random tie breaking. Notably, it iscan be easily inferred that the different seed value of a Random tiebreaking yield the different search behavior and different result. (In all of our experiment we fixed the seed to 1.)

Based on these observation, the next step we have taken is to develop a new
portfolio-based multi-tiebreaking strategy \textbf{which is orthogonal to
the approach of improving the heuristic accuracy.}




\section{Related Work}
\label{sec-4}

\emph{Symmetry Breaking} \cite{Fox1998,pochter2011exploiting,domshlak2013symmetry} is the search technique that tries to prune the states with symmetric paths. \emph{Partial Order Reduction}, \emph{Strong Stubbern Sets} and \emph{Expansion Core} are also the techniques which prune the intermediate states that reach to the same goal using the different orders of same actions. \emph{Dominance Pruning} \cite{erol1994} is a technique which exploits additional information from the problem after the heuristics are computed. Instead of computing the absolute distance, it  proves if a state is strictly relatively better than the other nodes. Since path similarity is weaker than symmetry or other criteria, our approach does not prune states, and instead just delay the expansion within the same f-value.

\section{Conclusion}

In this paper, we proposed two novel diversity-aware tie-braking methods for the admissible search using \astar. We empirically showed that they improve the performance on various domains, and they are heuristic-agnostic improvements. We showed that they have a significant impact on the final step of the search in large plateau.
 % when the distribution of optimal solutions is not uniform within the open list.
% We also showed that this nonuniform distribution still appears when we have almost-perfect % heuristics.

Our method differs from the other pruning techniques such as symmetry breaking, dominance pruning or partial-order-pruning because we actually do not prune any states, nor from the other general improvements in the heuristic accuracy because we just change the expansion order within the same $f$.
