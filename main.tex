\begin{abstract}
Despite the recent improvements in admissible heuristic search techniques
in classical planning, it is known that the the exponential growth of
search plateau in A* is unavoidable even under the optimistic assumption.
 % 
We investigate various existing myth on tiebreaking
 strategies and propose simple yet effective methods for improving the
 search performance within plateau.
 % 
 % 
 They do not depend on any particular heuristic, nor
 on multi-heuristic portfolio.
 They work even if the heuristic
 function no longer provides useful information.
 % Moreover, they do not even try to obtain any further information from
 % the domain.
 We empirically evaluate our strategies against state-of-the-art admissible planner.
\end{abstract}

Since the advent of delete-relaxation and abstraction heuristics in
admissible Classical Planning, much of the interest was focused on improving
the accuracy of these heuristic functions to prune more nodes from the
search space.
% 
However, recent work by Helmert and Roger
\shortcite{helmert2008good} claims that, even with an optimistic
assumption of \emph{almost-perfect heuristics}, \astar still has an
exponentially large plateau. They conclude that the further performance
improvement requires the techniques orthogonal to the heuristic
function, such as symmetry breaking, domain reduction, factored planning
etc.

The contribution of this paper is a new class of
tiebreaking strategy for \astar.
It is a heuristic-agnostic improvement and thus 
falls into above category of techniques.
It trivially maintains admissibility because it works only within the
same f-value and does not alter the expansion order wrto f-value.
% such technique.
% tiebreaking strategy for \astar.
% Our tiebreaking strategies fall into this category.

% The attention to the search algorithm itself is relatively small since
% most literature depends on bare \astar.  This was because variations of
% best first search focus on different requirements, such as IDA* on
% linear space, WA*/Lazy-A*/GBFS/EHC on trading speed for optimality, RWA*
% on replanning, etc., and not on improving upon \astar itself.

% maintaining all of its charactristics: memory-greedy yet efficient
% optimal algorithm.

More specifically, we show several important findings regarding the
existing tiebreaking strategy for \astar as follows.
% 
First, in implementing the open list of \astar, priority queue based on
LIFO-buckets is more efficient than that of FIFO-buckets.
% 
Second, with LIFO-based implementation, $h$-based tiebreaking which
frequently appears in the heuristic search literatures have little
impact on the performance.
% 
Third, the LIFO-based bucket implementation and $h$-based tiebreaking
both share the greedy search pattern within the plateau of the
search space, and thus one crucial essense of tiebreaking is the
\emph{depth within the plateau}.

The rest of paper is organized as follows: The next section describes the
preliminary background of \astar.
Next we compares several trivially-simple and well-known tiebreaking
methods on top of Fast Downward to show that even such a small
difference significantly affects the performance on domains with
large plateaus.
Next we propose a novel depth-aware tiebreaking methods and empirically
show that it outperforms previous strategies.
We finally conclude with a discussion on the future work.

\section{Backgrounds and Preliminary}
\label{sec-1}

\subsubparagraph{\astar and perfect heuristics}

\astar is a state of the art algorithm for finding an optimal path in the
search space represented as a graph. 
\astar returns an optimal solution when the heuristic function $h$ is
admissible, i.e., when it never overestimate the true distance to the goal
$h^*$.
% , and the nodes are first ordered according to $f$
% 
Thus, the best possible admissible heuristic function is $h^*$ itself, which is
called \emph{perfect heuristics}. However, computing $h^*$ is PSPACE-Complete,
which is as difficult as solving the problem itself and is not
practical.

%It is known that with a perfect heuristics the planner do not have to
% conduct any search: There are no multiple possibilities that the
% planner should examine on each node.

\emph{Almost perfect} heuristic function $h_c$ is a class of similar
impractical, theoretical functions which is also PSPACE-Complete to
compute \cite{helmert2008good}.  It has a constant error $c$ from the
perfect heuristic $h^*$, i.e., $h_c=h^*-c$.  The important finding by
\citeauthor{helmert2008good} is that even with this intractable and
impractical heuristic function, the number of the nodes in the last
plateau of the search becomes exponentially large as the problem size
increases.  Using this fact, they showed that relying only on the
improvement of the heuristic functions is not fruitful in the near
future, and the researchers should seek the other,
orthorgonal improvement.

These intractable heuristics are of course very hard and expensive to
compute. However, even the practical, tractable heuristic functions for
STRIPS Planning, such as \lmcut and IP-based heuristics, are so heavily
CPU intensive that they outweighs the space-greedy nature of
\astar. Also, compared to the other functions, these functions dominate
the search time over the other factors of planning algorithms, such as
node insertion and deletion.

% M\&S ?

\subsection{Tie Breaking Criteria in \astar}

% Compared to the other algorithms which
% guarantee optimality such as $IDA^*$ \cite{korf1985depth}, \astar requires a large amount of % memory to store the search nodes.

\astar stores the search nodes into two priority queues called an
\emph{open list} and a \emph{closed list}. In these queues, the search
nodes are sorted based on $f$ value, which is a sum of the actual cost
$g$ from the initial state and the cost estimate $h$ to the goal
state.
% On each iteration, \astar expands the open nodes with the
% smallest $f$ value and mark them as closed. Its successor nodes are
% accordingly inserted back to the open list or closed list, while
% sometimes the parent node is updated so that it has the shortest path
% from the initial state.

While the implementation of the priority queue varies, it has a degree
of freedom called tie-breaking, i.e. how to select the next node to open
within the same $f$.
In the current implementation of the \sota admissible planner Fast Downward (FD) \cite{Helmert2006}, the standard \astar search breaks ties based on $h$, meaning that if two nodes have the same $f$ value, the nodes with smaller $h$ will be selected, favoring the nodes with larger $g$ values (since $f=g+h$). The intension behind this is that the $h$ values are just an \emph{estimate} to the goal, while $g$ values are the \emph{actual} distance from the initial state and more reliable.

This tiebreaking strategy is very historically established and cited and
used in a lot of papers X, Y, Z.
Korf et al 

Another thing we note is that the nodes are stored in a FIFO order
within the same $h$ value. The planner selects the nodes that is inserted earlier. 
However, in fact, these tiebreaking methods are not necessary when we are only concerned with  maintaining the optimality. They are just the result of heuristic, ad-hoc selection by humans and have no theoretical background.
% In particular, we first observed that this FIFO order has not legitimate reason to support.

We therefore ran a preliminary experiments on 28 benchmark domains with
1294 problems in total, comparing simple FIFO, LIFO and Random-Order
second-level tiebreaking methods using 30 minutes runtime cutoff with
2GB memory limit.  All experiments below are conducted on Xeon
E5410@2.33GHz CPUs. We observed that even such a slightest difference
can change the performance significantly on some domains, shown in
\reftbl{f-h-coverage}. Due to the space limitation, we show only the
domains where the difference was observed. Full table is available in
the supplemental material.

\refig{f-h-eval} compares the number of
node evaluation (computations of \lmcut) by LIFO, FIFO and Random second
tiebreaking.  According to the figure, LIFO has smaller number of
evaluations than the others in Openstacks, but Random dominates the
others in Cybersec, indicating that there are no dominance relationship
between these three.  \reftbl{f-h-coverage} shows the coverage result
(number of problems solved) by each strategy.

Moreover, we also ran the same experiment but this time without $h$-based
first-level tiebreaking. Results in \reftbl{f-coverage} shows that the
coverage by LIFO-tiebreaking is almost comparable to those with $h$-based
tiebreaking, indicating that $h$-based tiebreaking is not essentially necessary.
This is a surprising result considering that almost all of the past literature assume the importance of the $h$-based tiebreaking and modern forward search planners employs this tiebreaking.

% Note that, although LIFO dominated the others, we consider this is just by a coincidence due to our selection of problems, time limit and domains. we \emph{are not trying to claim that any of LIFO or FIFO or Random order dominates the other}. However, there are noticeable performance difference cause by these different tiebreaking strategies.

% \begin{figure}[htbp]
%  \centering \relsize{-2}
%  \includegraphics{tables/aaai16-evaluated-lmcut_ff-lmcut_r.pdf}
%  \includegraphics{tables/opt11-evaluated-lmcut_ff-lmcut_lf.pdf}
%  \includegraphics{tables/opt11-evaluated-lmcut_lf-lmcut_r.pdf}
%  \caption{Comparison of the number of node evaluations (computations of
%  \lmcut) by FIFO, LIFO and Random tiebreaking, on IPC2011 optimal track
%  instance. LIFO order dominates FIFO and Random order especially in
%  openstacks instances, and the gap is more than one the order of 10.}
%  \label{single-eval}
% \end{figure}
% 
\begin{table*}[htbp]
 \centering \relsize{-3}
 \input{tables/aaai16-5min/1_1_vs_seedonly30-aaai16-prelim3-zerocost-concat.tex}
 \caption{Preliminary experiments comparing the performance of FIFO,
 LIFO and Random second-level tiebreaking using Fast Downward. Each cell
 denotes the problem solved with 30 minutes runtime, 2GB memory
 limitation. \textbf{Boldface} denotes the case where it achieved the
 best result among configurations.} \label{single-coverage}
\end{table*}

We also observed that, such differences occur especially in the problems which
have the huge search plateau, i.e., the problems where the heuristic
function is not informative and the planner relies heavily on the
tiebreaking criteria.
% 
\refig{plateau-f-h} shows the initial size of the
tiebreaking bucket in which the goal node was found, compared to the total number of evaluation.
Tiebreaking bucket is a set of open nodes in which the nodes share
the same $[f,h]$ value.
This is a result from [$f$,$h$,FIFO] tiebreaking, however in theory the size of the bucket
does not change among [$f$,$h$,FIFO], [$f$,$h$,LIFO] and [$f$,$h$,Random].
% 
% Since the nodes in a same bucket shares
% the same $[f,h]$ value, the planner cannot be guided by the
% heuristic functions within this bucket.
% 

According to the plot of Openstacks and Cyberspecs, 
the planner appears to spend almost half the runtime on searching through the final
plateau.
This indicates that these domains have very large variance in the time required to solve the problem because the runtime heavily depends on whether the goal nodes exist near the beginning of the bucket or near the end of the bucket.
In this kind of situation the second level tiebreaking like FIFO and LIFO greatly affects the performance.
\refig{plateau-h} is a similar plot result without the $h$-based tiebreaking.
As expected, much larger effort is spent on the final plateau without $h$-based tiebreaking.

\begin{figure}[htbp]
 \centering
 \relsize{-3}
 \includegraphics{tables/aaai16-front-vs-evaluated.pdf}
 \caption{Comparison of the size of the search plateau compared to the total evaluation. Data were obtained by the result of standard FIFO tiebreaking on the standard benchmark instances. Both axes are logarithmic. Each dotted line represents 10x, 100x ... lines.  Openstacks,  clearly has the large plateaus.}
 \label{plateau-h}
\end{figure}
% 
% \begin{figure}[htbp]
%  \centering
%  \relsize{-2}
%  \includegraphics{tables/aaai16-front-vs-evaluated.pdf}
%  \caption{Comparison of the size of the search plateau compared to the total evaluation. Data were obtained by the result of running \astar on the standard benchmark instances, with FIFO but without the tiebreaking by $h$. Both axes are logarithmic. Each dotted line represents 10x, 100x ... lines.}
%  \label{plateau-f}
% \end{figure}

We can have several important observations from these results.  Firstly, in a plateau, \textbf{the heuristic functions are not used at all, nor the search is guided at all}. This observation holds even if we combine several nondominating heuristics e.g. \lmcut and M\&S, regardress of the method, e.g., taking the maximum, using portfolio or utilize them as the first tiebreaking strategy. It is still possible that a plateau is encountered, since it is not a perfect heuristics yet!
Such a plateau is known to be inevitable even if we have an almost perfect heuristics $h_c$, and it is impossible to improve upon $h_c$ --- if it could, the result would be a perfect heuristics or an inadmissible heuristics. Therefore, this problem cannot be solved by improving the heuristic accuracy, which is the currently dominating meta-strategy to improve the planner performance.

Secondly, there is no legitimate reason which supports each tiebreaking strategy.
As long as $f$-value is the first sort key, any tiebreaking criteria are admissible. \textbf{$h$ and FIFO are just heuristically chosen by the implementer of the planner.} Nor are there any reason to choose LIFO or Random tie breaking. Moreover, the different seed value of a Random tiebreaking yield the different search behavior and different result. (In all of our experiment we fixed the seed to 1.)

% Based on these observation, the next step we have taken is to develop a new
% portfolio-based multi-tiebreaking strategy \textbf{which is orthogonal to
% the approach of improving the heuristic accuracy.}

The following sections are devoted to giving further analysis on the reason behind their behavior.

\section{Domains with Large Plateau}

Currently, most benchmark domains except Openstacks and Cybersec do not
have the large plateau thanks to the powerful heuristic
estimates. However, limiting our effective experiments only to 2 domains
would bias our observation. To avoid this issue, we created several
domains where the \sota heuristic functions fail to provide a
menaingful guidance.

One important characteristics shared by Openstacks and Cybersec is that they both
have large number of zero-cost actions. In such situations, both LMcut
and M\&S fail to find a meaningful heuristic estimate because LMcut fails to
find a good cost partitioning with non-zero values, and most edges in the abstraction space of
M\&S have zero costs.

We therefore modified various domains to have many zero-cost actions.
For example, miconic-up is a domain which minimizes the energy
consumption caused by ``up'' action, which moves the elevator up, and
all other actions have zero-cost. Another example is driverlog-fuel, where only
the ``drive'' action has cost 1 and all other actions are zero-cost.
This in fact reflects the practical application compared to the original
unit-cost domains where driving and manual labor is equally accounted.
Oddly, although some planners have options which treats actions as if
they are unit-costs, and describe such options as ``inadmissible'',
solving domains which are unit-cost by origin is not called
``inadmissible''. Above domain modification addresses this problem.

Modification was mainly conducted so that it is practically reasonable
in a sense of cost minimization. Most transportation-type domains are
modified so that they use less fuel. Assembly-type domains are modified
so that it minimizes the resource usage such as ink or wood.

\section{Depth-based Tiebreaking}

In order to solve this kind of problem with a large final plateau, the
planner needs to run an efficient knowledge-free search within the plateau.
One useful measure for such situation is the number of steps from
the entrance of the plateau.

The \emph{depth} of a node is an integer equal to the depth of its
parent node plus one. If the parent node is from the other plateau,
e.g., different $f$-value, or different $h$-value used for the first
tiebreaking, the depth is 0.

With this simple notion of depth, we developed the third level tiebreaking method that stores the nodes in the different buckets each associated with particular depth.  This method is very similar to inadmissible search technique in LAMA planner \cite{richter2010lama} which increases every action costs by 1, called PLUSONE cost-type.  It is explicitly targeted at zero-cost actions observed in Openstacks, and resulted in a significantly better performance in IPC-6.  The major difference of our depth-based tiebreaking from there strategy is twofold.  First, the depth used for tiebreaking does not affect the cost, thus does not lose the admissibility. Next, we \emph{do not favor smaller depth over higher depth} --- In PLUSONE, three successive applications of zero-cost operators result in cost 3, and two applications result in a cost 2, and smaller cost is preferred, just as \astar always expands the node with smaller $f$-value.  However, in the knowledge-free search within the plateau of admissible search, all nodes have the same $f$-value and it is impossible to guess whether the goal is near the entrance or far away from the entrance.  In the formar case, the search should be focused around the entrance favoring the smaller depths, and the behavior is much like Breadth First Search. However, in the latter case, the planner should greedily explore the various area of the plateau by preferring larger depth, much like a Depth First Search.
This ``greediness'' is different from the normal sense of ``greedy search'' --- since this greediness only holds within the plateau, admissibility is still maintained.

\section{MultiSearch Engine}

As noted previously, it is unknown prior to the search whether the goal is near the entrance of the plateau or not. In particular, when almost perfect heuristics was used and a plateau is encountered, it is very unlikely that there is a room of obtaining further information from the problem. 
% 
However, it would be affected by the domain and problem.
in fact, \refig{xxx} shows that the second-level random tiebraking significantly outperforms LIFO tiebraking in Cyberspecs, YYY, ZZZ.

Based on this fact, 






We call our new search algorithm within \astar a \emph{MultiSearch}.
It simulates multiple search engines using the same heuristic functions,
but with the different tiebreaking strategies.  Each engine has completely
separate open list and closed list.  However, there is a
globally shared hash table which fully caches the result of heuristic
functions.  Whenever a search engine expands a state, it checks if the
result of a heuristic is already computed, and if yes, it reuses the
result.  Each search engine expands a state in turns, sequencially. The algorithm
finishes when some engine finds the solution.

Assume now we use two \astar engines, both using $f=g+h$ where $h=$\lmcut, both using $h$ as the first tiebreaking, and each using FIFO and LIFO as the second tiebreaking.
The amount of memory used for the open/closed list is doubled, and the effort to push/pop the search nodes is also doubled.
However, the computation of \lmcut is so heavy that those wasted efforts are negligeble.
We first verified this by running a MultiSearch search engine with two same search engines, each using \lmcut and FIFO queue, and compared its runtime agains the single engine using \lmcut and FIFO. The result in \refig{ffff} shows that the extra cost of duplicated effort is negligeble.

\begin{figure}[htbp]
 \centering
 \relsize{-2}
 \includegraphics{tables/opt11-time-lmcut_ff-lmcut_ffff.pdf}
 \caption{Comparison of runtime on problems solved by both single FIFO search engine (ff) and a MultiSearch engine with 2 different instances of the same FIFO engine (ffff). The runtime difference was on average below a factor of x1.1, if we ignore the subsecond differences.}
 \label{ffff}
\end{figure}

This portfolio strategy has several interesting theoretical characteristics. First, if we ignore the negligeble cost of insertion and deletion to the open/closed list, we do not have to pay the extra cost evaluating the heuristic function for states $f<f^*$ thanks to the caching.
Recall that \astar always has to expand the states whose $f$ values are below $f^*$, the true distance from the initial state to the goal. If the heuristic estimate $h$, and in turn $f=g+h$ is the same, any tiebreaking strategy expands and evaluates the same set of nodes in $f<f*$.
Therefore, in region $f<f*$, our caching mechanism fully works and completely eliminates the possibility of extra evaluation caused by adding another queues.

Second, since the search terminates when \emph{some} engine finds a solution, and since the expansion happens in turns, the search effort within the final plateau is upper-bound by \emph{twice} the \emph{minimum} of the search efforts required by LIFO or FIFO engine. This is desirable because, as we saw in the last section, in some domains the gap between the best and worst tiebreaking strategy can be more than 10 times (Openstacks, for example).
When there are $n$ engines, then this increases to $n\times$ minimum amount of effort by each single engine.

Finally, we conducted experiments for evaluating our MultiSearch strategy.
% the number of evaluations and expansions, along with 
\refig{portfolio-ff} to \refig{portfolio-r} shows the runtime between different combinations of 2 or 3 tiebreaking strategies (FIFO+LIFO, FIFO+Random, LIFO+Random, FIFO+LIFO+Random) and the single tiebreaking strategies. The results support our claim that the evaluation never exceeds twice/thirds of the single search engine and, in practice, the evaluations are mostly the same with the single search engine, and in some domains with large plateaus, the search effort used by MultiSearch is more than ten times less than by the single strategy.

\begin{figure}[htbp]
 \centering
 \relsize{-2}
 % \includegraphics{tables/opt11-evaluated-lmcut_ff-lmcut_fflf.pdf}
 % \includegraphics{tables/opt11-evaluated-lmcut_ff-lmcut_ffr.pdf}
 % \includegraphics{tables/opt11-evaluated-lmcut_ff-lmcut_fflfr.pdf}
 \caption{}
 \label{portfolio-ff}
\end{figure}



\section{Related Work}
\label{sec-4}

\emph{Symmetry Breaking} \cite{Fox1998,pochter2011exploiting,domshlak2013symmetry} is the search technique that tries to prune the states with symmetric paths. \emph{Partial Order Reduction}, \emph{Strong Stubbern Sets} and \emph{Expansion Core} are also the techniques which prune the intermediate states that reach to the same goal using the different orders of same actions. \emph{Dominance Pruning} \cite{erol1994} is a technique which exploits additional information from the problem after the heuristics are computed. Instead of computing the absolute distance, it  proves if a state is strictly relatively better than the other nodes.

LA* is an extension of \astar which employs a \emph{lookahead} to each
expansion of a node. Lookahead is a Depth First Search from the frontier
node which allows to find nodes whose 

\section{Conclusion}

In this paper, we proposed two novel diversity-aware tie-braking methods for the admissible search using \astar. We empirically showed that they improve the performance on various domains, and they are heuristic-agnostic improvements. We showed that they have a significant impact on the final step of the search in large plateau.
 % when the distribution of optimal solutions is not uniform within the open list.
% We also showed that this nonuniform distribution still appears when we have almost-perfect % heuristics.

Our method differs from the other pruning techniques such as symmetry breaking, dominance pruning or partial-order-pruning because we actually do not prune any states, nor from the other general improvements in the heuristic accuracy because we just change the expansion order within the same $f$.
