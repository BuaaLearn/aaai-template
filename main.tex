% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{comment}
% 
% * Maintain 80 characters / line.
%  
% * too much ``''s make the sentence look scattered and visually less recognizable. ``e.g.'' also.
% 
% * \em, \bf, \it are all obsolete \TeX primitives, and it does not take effect properly --- for example, {\bf {\it aaa}} shows ``aaa'' in italic but NOT IN BOLD. Use \emph{}, \textit{}, \textbf{} and so on.
% 
% * always use \ff, \fd, \cea, \pr, \mv , and do not use it directly, e.g. FF, FD/LAMA2011, etc. 
% 
% * use of footnotes should be minimized.
% 
% * IPC2011 should always be \ipc . The definition can later be modified in abbrev.sty .
% 
% * prefer separated words over hyphened words. domain
%   independent>domain-independent, planner independent >
%   planner-independent.
% 
% * Table, Figure, Fig., should not be used directly. Always use \refig and \reftbl. When the development flag is enabled, direct use of \ref signals an error.
% 
% * Caption ends with a period.
% \end{comment}
% 
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 
\begin{abstract}
While classical planning has been an active branch of AI,
its applicability is limited to the tasks precisely modeled by humans.
Fully automated high-level agents should be instead able to find a symbolic representation
of an unknown environment without supervision,
% i.e., no help from manually-assigned labels nor predefined reinforcement signals,
otherwise it exhibits the knowledge acquisition bottleneck.
% 
Meanwhile, \latentplanner \cite{Asai2018} partially resolves the bottleneck
with a neural network
called State AutoEncoder (SAE). SAE obtains the propositional
representation of the image-based puzzle domains with unsupervised learning,
generates a state space and performs classical planning.
% 
In this paper,
we identify the problematic, stochastic behavior of the SAE-produced propositions 
as a new sub-problem of symbol grounding problem, the symbol \emph{stability} problem.
Informally, symbols are
\emph{stable} when their referents (e.g. propositional values) do not change against small perturbation of the observation,
and unstable symbols are harmful for symbolic reasoning.
We analyze the problem in Latplan both formally and empirically, and
propose ``Zero-Suppressed SAE'', an enhancement that stabilizes the propositions
%% Add this back in the camera-ready version
using the idea of closed-world assumption as a prior for NN optimization.
% 
We show that
it finds the more stable propositions and the more compact representations, resulting in an improved success rate of \latentplanner.
It is robust against various hyperparameters and eases the tuning effort, and also provides a weight pruning capability as a side effect.
\end{abstract}


\section{Introduction}

% [This second version of introduction is too long]

Symbol grounding problem \cite{harnad1990symbol,Steels2008} is one of the key milestones in AI research
which seeks to achieve high-level intelligence.
In Physical Symbol Systems Hypothesis \cite{newell1976computer}, it is believed that
an agent with high-level intelligence performs tasks by efficiently manipulating a compact set of abstract symbols.
% 
Symbolic manipulation
% consists of mechanical applications of composable rules which 
allows for
the development of highly optimized and generalized, domain-independent heuristics \cite{Hoffmann01,Helmert2009}
% (e.g. FF \cite{Hoffmann01} or LMcut \cite{Helmert2009})
that can be easily applied to multiple tasks with few or no data,
while the current learning-based approaches struggle to improve its multi-task performance and data efficiency.
% For example, training AlphaGo \cite{alphago} takes days on a huge cluster of custom-made Tensor Processing Units.
% 
To enable such a symbolic computation in a real-world environment,
agents should be able to find the symbolic representation of the environment by itself.

Recently, Latplan system \cite{Asai2018} successfully
connected a subsymbolic neural network (NN) system and a symbolic Classical Planning system
to solve various visually presented puzzle domains.
% In contrast to the work by Konidaris et. al. (\citeyear{KonidarisKL14}),
% Latplan is a straightforward NN built upon a \lsota framework.
The State AutoEncoder (SAE) neural network in \latentplanner
generates a set of propositional symbols from the training images with no additional information
and provides a bidirectional mapping between images and propositional states.
% 
The system then solves the propositional planning problem using a classical planner Fast Downward \cite{Helmert04}
and returns an image sequence that solves the puzzle
by decoding the intermediate propositional states of the plan.
It also discovers a set of action symbols that distinguish the modes of
state transitions through AMA$_2$ unsupervised learning process.
Thus the system grounds two kinds of symbols:
Propositional symbols and action symbols,
% 
and opens a promising direction for applying a variety of symbolic methods to the real world.
The search space generated by \latentplanner was shown to be compatible
to a symbolic Goal Recognition system \cite{amado2018goal}.
Another approach replacing SAE/AMA$_2$ with InfoGAN was also proposed recently \cite{kurutach2018learning}.
% although it lacks the ability to find a finite set of action labels and resort to sample the successor states
% without guaranteeing to enumerate the successors.

Despite its success,
% the propositional representations learned by SAE have a problematic behavior
%%%%%% SAE -> NN because we also want to cover the InfoGan case
the propositional representations learned by SAEs have a problematic behavior
due to its lack of strong guarantees on the learned results.
% 
That is, while the SAE can reconstruct the input with high accuracy,
the learned latent representations are not ``stable'', i.e.,
some propositions may flip the value (true/false) randomly
given the identical or nearly identical image input.
% \footnote{
% The reason the randomness does not harm the reconstruction accuracy is that
% the later pipeline of the network learns to ignore the random neurons by assigning a negligible weight.
% }
% only slightly perturbed image input.
% 
% Prevent reviewers from reaching the early conclusion that this is about robustness.
% 
% We also found out that instability does not harm the reconstruction accuracy of SAE.
% This indicates that \emph{unstable} propositions are also ``unused'' and ``uninformative''.
% ,
% just as the boy who cried wolf was ignored by the villagers.
% in the Aesop's Fables.
This is mainly because SAE learns the mapping between images and binary representations as a many-to-many relationship.
While this property has not been considered as an issue in the machine learning community where only the accuracy matters,
its unstable latent representation poses a significant threat to the reasoning ability of the planners.

Unstable symbols are harmful for symbolic reasoning because
they break the identity assumption built into the reasoning algorithms.
For instance, in \latentplanner, 
a single image may map to multiple propositional states due to stochasticity;
therefore the duplication detection in search algorithms such as \astar \cite{hart1968formal}
fails to realize that a single real-world state is visited multiple times through
different symbolic state representations.
% 
Unlike machine learning tasks,
% the mapping between images and the binary representation could be a many-to-many relationship, while
the symbolic planning requires a mapping
that abstracts many images into a single symbolic state, i.e., many-to-one mapping.
% 
To this end, it is necessary to develop an autoencoder that learns a
many-to-one relationship between images and binary representations.

\begin{figure}[tb]
 % It is better to have this figure in the first page --- whatever figure it is, it hooks
 % reader's interest
 \centering
 \includegraphics{img/zsae-overview.pdf}
 \caption{
Autoencoding results of an MNIST 8-puzzle state
using a vanilla State AutoEncoder (SAE) \cite{Asai2018} and a proposed Zero-Suppressed SAE (ZSAE) with 100 propositions.
ZSAE obtains a compact representation that uses fewer true bits.
}
 \label{zsae-overview}
\end{figure}

The contribution of this paper is threefold.
% This problem arises for any grounding processes applied to the real world.
The first contribution of this paper is the identification of a sub-problem of symbol grounding
called ``symbol \emph{stability} problem'' (SSP), which seeks to find a set of symbols
whose values/referents stays the same for the same/similar raw inputs.
Stability is orthogonal to the \emph{performance/accuracy} of NNs:
NNs are known to predict the correct output robustly, but they do not guarantee
the quality of the internal representation.
It is also unrelated to the numerical stability of the training or the reproducibility of the results.

The second contribution of this paper is the formal analysis of the Latplan system,
in particular, the comparison to the original mathematical model of Gumbel-Softmax
\cite{jang2016categorical}.
The analysis revealed that Latplan's Gumbel-Softmax has a deviation from
the original, which was the key to its first success: The slight
modification acted as an \emph{Entropy Regularization} term for the SAE neural
network that suppresses the randomness to some extent.
%% It is me, Masataro :)
% Through a personal communication, Latplan's authors were not aware that they made this modification.

The third contribution of this paper is
the proposal of Zero-Suppressed State AutoEncoder (ZSAE, \refig{zsae-overview}).
Inspired by the fact that the \emph{Entropy Regularization} stabilizes the representation,
ZSAE further stabilizes the propositions 
by an additional regularization term which
guides the network optimization so that unused propositions tend to 
take the value of zero (false) instead of random values.
% 
The stable representation results in a higher success rate of classical planning.
Also, the network is less sensitive to the network size (hyperparameters)
as it automatically reduces the number of bits used.
Moreover, we show that we can reduce the memory usage of the network
by pruning some unused neurons
that now have a constant activation of zero instead of random values.

% The rest of the paper is organized as follows.
% In \refsec{background}, we introduce Latplan \cite{Asai2018} and its SAE.
% Next, we describe a problematic behavior of this vanilla SAE and
% a broader problem of \emph{Symbol Stability Problem} (\refsec{issues}).
% We next analyze the vanilla SAE to understand the source of this behavior (\refsec{analysis}).
% Next, we introduce Zero-Suppressed SAE (ZSAE), the main contribution of this paper that addresses this problem (\refsec{zsae}).
% We empirically evaluate the stability of the propositions generated by the vanilla SAE and the ZSAE,
% as well as various other advantages of ZSAE (\refsec{evaluation}).
% We finally conclude the paper with related work and future remark (\refsec{conclusion}).


\section{Preliminaries}
\label{background}

\textbf{Symbol grounding} is an unsupervised process of establishing a mapping
from huge, noisy, continuous, unstructured inputs
to a set of compact, % clean
discrete, identifiable (structured) entities, i.e., symbols \cite{harnad1990symbol,Steels2008,Asai2018}.
PDDL \cite{McDermott00} has six kinds of symbols: Objects, predicates, propositions, actions, problems and domains.
Each type of symbol requires its own mechanism for grounding.
For example, the large body of work in the image processing community on recognizing 
objects (e.g., faces) and their attributes (male, female) in images, or scenes in videos (e.g., cooking)
can be viewed as corresponding to grounding the object, predicate and action symbols, respectively.
In this paper, we focus on grounding the propositional symbols.

\textbf{\latentplanner} \cite{Asai2018} is a framework for
\emph{domain-independent image-based classical planning}.
\latentplanner is able to ground
the propositional and action symbols.
% 
Classical planners such as FF \cite{Hoffmann01} or
FastDownward \cite{Helmert04} takes a PDDL model as an input, which
% , the initial state, the goal condition
specifies the state representation and the transition rules.
In contrast, \latentplanner learns the state representation as well as the transition rules
entirely from the image-based observation of the environment with deep NNs.
% , and also claims to extend its capability on text/audio-based dialog data in the future.
The system was shown to solve various puzzle domains, such as 8-puzzles or Tower of Hanoi,
that are presented in the form of noisy, continuous visual depiction of the environment.

% \begin{figure}[tb]
%  \centering
%  \includegraphics[width=\linewidth]{img/planning.pdf}
%  \caption{Classical planning in latent space:
% It uses the learned State AutoEncoder (\refig{sae}) to convert pairs of images $(\before,\after)$ to symbolic transitions,
%  from which the Action Model Acquisition (AMA) component generates an action model.
% A classical planner finds the symbolic solution plan for symbolic initial/goal states encoded from images.
% Finally, intermediate states in the plan are decoded back to a human-comprehensible image sequence.}
% \label{fig:overview}
% \end{figure}

% \latentplanner (\refig{fig:overview}) takes two inputs.
\latentplanner takes two inputs.
The first input is the \emph{transition input} $Tr$, a set of pairs of raw data.
Each pair $tr_i=(\before_i, \after_i) \in Tr$ represents a transition of the environment before and after some action is executed.
The second input is the \emph{planning input} $(i, g)$, a pair of raw data, which corresponds to the initial and the goal state of the environment.
The output of \latentplanner is a data sequence representing the plan execution that reaches $g$ from $i$.
While the original paper uses an image-based implementation (``raw data'' = images),
the type of data is arbitrary as long as it is compatible with NNs.

% Emphasize the type of symbol, as readers are not familier with or haven't deeply thought about symbols

\latentplanner works in 3 phases.
In Phase 1, a \emph{State AutoEncoder} (SAE) (\refig{sae}) learns a bidirectional mapping between raw data (subsymbolic representation e.g., images)
 and propositional states (symbolic representation) from a set of unlabeled, random snapshots of the environment.
% The $Encode$ function maps images to propositional states, and $Decode$ function maps the propositional states back to images:
The trained SAE provides two functions:
\begin{itemize} %this is crucial to understanding the rest of the paper, so making an itemize to highlight it
\setlength{\itemsep}{-0.3em}
\item $b=Encode(r)$ maps an image  $r$ to a boolean vector $b$.
\item $\tilde{r}=Decode(b)$ maps a boolean vector $b$ to an image $\tilde{r}$.
\end{itemize}
After training the SAE from $\braces{\before_i, \after_i\ldots}$,
it applies $Encode$ to each $tr_i \in Tr$ and obtains $(Encode(\before_i),$ $Encode(\after_i))=$ $(s_i,t_i)=$ $\overline{tr}_i\in \overline{Tr}$,
the symbolic representations (latent space vectors) of the transitions.

In Phase 2, an Action Model Acquisition (AMA) method learns an action model (e.g., PDDL, successor function) from $\overline{Tr}$ in an unsupervised manner.
The original paper proposed two approaches: AMA$_1$ is an oracle which directly generates a PDDL without learning,
by allowing it to use the whole set of valid transitions as an oracle.
In contrast, AMA$_2$ approximates AMA$_1$ by unsupervised learning from examples.

In Phase 3, a planning problem instance is generated from the planning input $(i,g)$.
These are converted to the symbolic states by the SAE, and the symbolic planner solves the problem
combining $(i,g)$ and the generated action model.
For example, an 8-puzzle problem instance consists of an image of the start (scrambled) configuration of the puzzle ($i$), and an image of the solved state ($g$).

Since the intermediate states comprising the plan are SAE-generated latent bit vectors, the ``meaning'' of each state (and thus the plan) is not clear to a human observer.
However, in the final step, \latentplanner obtains a step-by-step visualization of the plan execution
by $Decode$'ing the latent bit vectors for each intermediate state.
% This necessitates the bidirectionality of the mapping between the input and the propositional states.
The plans or the state transitions are validated based on the visualized result using a custom domain-specific validator implemented in Latplan.
This is because the intermediate latent representation is learned unsupervised
and is not directly verifiable through human knowledge.

\begin{figure}[tb]
 \includegraphics[width=\linewidth]{img/train-state-ae.pdf}
 \caption{Step 1:
Train the State AutoEncoder by
 minimizing the sum of the reconstruction loss and the variational loss of Gumbel-Softmax.
As the training continues, the output of the network converges to the input images.
Also, as the Gumbel-Softmax temperature $\tau$ decreases during training,
the latent values approach either 0 or 1.}
 % \caption{State AutoEncoder, a
 % Variational AutoEncoder \cite{kingma2014semi} using Gumbel-Softmax \cite{jang2016categorical} reparametrization in its
 % latent layer.}
 \label{sae}
\end{figure}

\subsubsection{SAE as a Gumbel-Softmax Variational AutoEncoder}

The key concept of the SAE in \latentplanner is the use of Gumbel-Softmax \cite{jang2016categorical}
in the latent activation of the variational autoencoder~(VAE).
This allows the SAE to obtain a discretized binary representation, and \latentplanner uses this
discrete vector as the state representation for classical planning.
We briefly review the related literature below.

An autoencoder~(AE)~\cite{hinton2006reducing} is a feed-forward NN that consists of a pair of encoder and decoder networks, both of which are modeled by continuous functions.
For example, in Figure~\ref{sae}, the mapping from the leftmost image to the vector in the middle corresponds to the encoder, and the mapping from the middle to the rightmost image corresponds to the decoder.
AEs are trained so that the encoder maps a data point~(e.g., an image) into a low-dimensional latent space, and the decoder pulls the latent representation of the input back to the original data point.
Technically, they are trained by a \emph{backpropagation} algorithm so as to minimize the reconstruction loss, the distance between the input and the output measured by Euclidean distance or binary cross entropy.
Since the encoder is modeled by a continuous function, its latent representation is also continuous, which makes it challenging to integrate AEs with propositional reasoners.
% \if0
% Since it is trained solely by the raw input data, it is a unsupervised learning method that do not require the human-assigned labels.
% Its intermediate layer (typically smaller than the input) has a compressed, \emph{latent representation} of the input.
% % AEs are commonly used for pretraining a NN.
% NNs, including AEs, typically have continuous activations and integrating them with propositional reasoners is not straightforward.
% \fi

A variational autoencoder (VAE) \cite{kingma2013auto} is a probabilistic variant of AEs, whose encoder and decoder are modeled by probabilistic distributions rather than deterministic functions.
For instance,
it obtains a discrete latent representation of the data by modeling the Bernoulli(=binary) random distribution with the encoder.
VAEs are trained by backpropagation with the help of the reparameterization trick, which makes random variables differentiable.
% Trying to avoid saying ``Gumbel-Softmax is approximating Bernoulli'', since it is in fact approximating Categorical, but we want to avoid mentioning Categorical
The reparametrization trick for the Bernoulli distribution is Gumbel-Softmax~\cite{jang2016categorical}.
% This approximation is necessary because the reparameterization trick is only applicable to the continuous cases.

% that forces the \emph{latent layer} (the most compressed layer in the AE) to follow a certain distribution (e.g., Gaussian).
% While initially proposed for enforcing Gaussian distributions, VAEs have been used to enforce arbitrary types of distribution (notably by Generative Adversarial Network \cite{goodfellow2014generative,makhzani2015adversarial}). 

A single Gumbel-Softmax unit in the Neural Network is able to model a categorical distribution with $M$ categories
which includes a Bernoulli distribution ($M=2$) as a special case.
Latplan uses $M=2$ for simplicity, and it does not affect the expressivity of the representation,
similar to the relation between SAS+ and the propositional representation.
% 
\begin{align}
z_i &= \textbf{if}\ (i \ \text{is}\ \arg \max_i (g_i+\log \pi_i)) \ \textbf{then}\ 1\ \textbf{else}\ 0. \label{eq:gumbelmax} \\
z_i &= \text{Softmax}((g_i+\log \pi_i)/\tau).                   \label{eq:gumbelsoftmax}
\end{align}
% 
The output of a single Gumbel-Softmax unit $GS(\boldsymbol{\pi}) = \mathbf{z}=(z_i)$ $(0\leq i \le M)$ is a one-hot vector representing $M$ categories, e.g.,
when $M=2$ the categories can be seen as $\braces{\text{false},\text{true}}$ and $\mathbf{z}=\parens{0,1}$ represents ``true''.
(Note: There is no explicit meaning assigned to each category.)
The input $\boldsymbol{\pi}=(\pi_i)$ is a class probability vector, e.g. $\parens{.2,.8}$.
Gumbel-Softmax is derived from Gumbel-Max technique \cite[\refeq{eq:gumbelmax}]{maddison2014sampling}
for drawing a categorical sample from $\boldsymbol{\pi}$
where $g_i$ is a sample drawn from
 $\text{Gumbel}(0,1) =-\log (-\log u)$ where $u=\text{Uniform}(0,1)$ \cite{gumbel1954statistical}.
Gumbel-Softmax approximates the argmax with a softmax to make it differentiable (\refeq{eq:gumbelsoftmax}).
``Temperature'' $\tau$ controls the magnitude of approximation, which is annealed to 0 by a certain schedule.
The output $\mathbf{z}$ converges to a discrete one-hot vector when $\tau\rightarrow 0$.

SAEs have $N$ Gumbel-Softmax units
% of $|D|=2$ categories ($D=\braces{\text{false},\text{true}}$)
to model an $N$-dimensional Bernoulli/boolean/propositional variable $\mathbf{b}=(b_n)$.
$N$ units produce a matrix $z_{nk}$ where $1\leq n \leq N$ and $k\in\braces{0,1}$ and
the boolean variables are retrieved by $b_n=z_{n1}$.
% 
% In the vanilla SAEs \cite{Asai2018}, the output layer of the encoder consists of $N$ units of Gumbel-Softmax
% % that approximates the Bernoulli distribution, %, resulting in a $N\times 2$ matrix,
% and therefore, the latent representation can be regarded as $N$ binary propositional variables.
% 

% It is necessary for the mapping to be bidirectional, because the
% propositional variables are machine-generated symbols, and also the
% resulting plan (returned as a sequence of propositional states) should
% be decoded back to real-world images which are interpretable for humans.

% \latentplanner also learns the action model using a neural action model acquisition method AMA$_2$,
% but we omit the details due to space limitation.

\section{Symbol Stability Problem}
\label{issues}

The vanilla SAE in \latentplanner can map a visual observation of the environment to/from a set of propositional values.
An issue with the vanilla SAEs is that the class probability for the class ``true'' and the class ``false''
that is mapped to by the Gumbel-Softmax could be neutral at some neuron,
causing the value of the neuron to change frequently (\refig{unstable}).
The source of stochasticity is twofold.
The first source is the probabilistic distribution modeled by the encoder,
which introduces stochasticity and causes the propositions to change values even for the exact same inputs.
The second source is the stochastic observation of the environment which corrupts the input image.
When the class probabilities are almost neutral,
such a tiny variation in the input image may cause the activation to go across the decision boundary for each neuron,
causing the bit flips.
In contrast, humans still regard the corrupted image as the ``same'' image.

\begin{figure}[tb]
 \centering
 \includegraphics{img/unstable.pdf}
 \includegraphics{img/disconnected.pdf}
 \caption{
(\textbf{Top}) Propositions found by SAEs may contain uninformative random bits
 that do not affect the output.
(\textbf{Bottom})
 The random variations of the propositional encoding could disconnect the search space.
}
 \label{unstable}
 \label{disconnected}
\end{figure}

This stochastic behavior of the propositional representation
introduces several issues to the recipient symbolic systems such as classical planners.
% 
Firstly,
search algorithms that run on the state space generated by the propositional vectors
are confused by many variations of the essentially identical real-world states.
It could visit the ``same'' real world state several times because
it could be encoded into different propositional vectors
which are not detected by the duplicate detection in the search algorithms (e.g. \astar).
% since it completely relies on the atomiticy and deteminisim of the proposition.
This slows down the search by increasing the number of nodes that are reachable from the initial state.

Secondly, the state space could be disconnected due to such random variations (\refig{disconnected}).
Some states may be reached only via a single variation of the real world state and is not connected to
another propositional variation of the same real-world state.
In fact, in the appendix section in the Arxiv version of the original paper \cite{Asai2018},
the planning part used a so-called \emph{state augmentation} technique
which circumvents this issue by sampling states from the same image multiple times.

Thirdly, in order to reduce the stochasticity of the propositions, we encounter a hyperparameter tuning problem
which is costly when we train a large NN.
% 
The neurons that behave randomly for the same or perturbed input do not affect the output,
i.e., they are unused and uninformative.
Unused neurons appear because the network has an excessive capacity to 
model the entire state space, i.e., they are surplus neurons.
Therefore, a straightforward way to reduce those neurons is to reduce the size of the latent space $N$.
On the other hand, if $N$ is too small, it lacks the capacity to represent the state space
and the SAE no longer learns to reconstruct the real world image.
As a result, we face a hyperparameter tuning problem: Surplus $N$ causes an unstable representation,
and insufficient $N$ makes the network hard to train.

% As we have seen, unstable propositional representations are harmful to symbolic reasoning algorithms (search)
% in various ways.
Fundamentally,
the first two harmful effects are caused by breaking a critical feature of symbols, \emph{designation} \cite{newell1976computer},
% ,newell1980physical --- removed for space
that each symbol uniquely refers to an entity (referent, concept, meaning),
e.g., the referents of the symbols grounded by SAEs are the truth assignments.
If meaning of a symbol changes frequently and unexpectedly, the entire symbolic manipulation is fruitless
because the underlying symbols are not tied to any particular concept, and does not represent the real world.
% just as the famous boy in the Aesop's Fables who cried wolf was ignored by the villagers.
% [more intuitive example?]
% A famous 
% Colorless green ideas sleep furiously


Thus, for a symbol grounding procedure to produce a set of symbols for symbolic reasoning,
it is insufficient to find a set of symbols that are \emph{just able to} represent the environment;
It should find a \emph{stable} symbolic representation that \emph{uniquely} represents the environment.
% i.e., to solve the \emph{Symbol Stability Problem}.
% This stability should be checked 

\begin{defi}
\emph{Symbolic representation} of an environment is a set of symbols with referents
from which the environment can be reconstructed with sufficient accuracy.
\end{defi}

\begin{defi}
Symbolic representation is \emph{stable} when its referents are identical
 %  (e.g. truth assignment for propositional symbols)
for the same environment, under some equivalence relation (e.g., invariance, noise threshold).
\end{defi}

%% wanted to include, but not during the submission
% The meaning of ``the same observation'' depends on the context,
% and the acceptable divergence within the set of actual data for ``the same observations'' may be either low-level or high-level.
% Low-level divergence includes the noise in the images,
% while high-level divergence may include any task-specific inessential details.
% For example, in the visual depiction of a STRIPS Blocksworld,
% the actual locations of the stacks would be irrelevant to the task.

%% single example is not enough
% From a practical perspective, 
% the impact of unstable symbols on symbolic reasoning systems is typically exponential to the number of unstable symbols.
% For example, in the case of search algorithms, each unstable symbol doubles the size of the state space (being true or false).
% % In a hypothetical Prolog-like logic programming system, each unstable predicate symbol would also double the
% % number of unifications.

While NNs tend to achieve a robust performance on noisy data,
\textbf{the \emph{robust performance} and the stability of the representation are orthogonal.}
This is because the former exclusively deals with the output accuracy,
while the latter evaluates the quality of the \emph{latent} activations while maintaining the same output accuracy.
In fact, vanilla SAEs already achieve the almost perfect reconstruction accuracy
where the input and the output are indiscernible to human eyes
(e.g., in \refig{unstable}, the pixel value output is different from the input, but we cannot recognize it),
while they still exhibit instability.

% stability

The stability of the representation obtained by a NN depends
on
its inherent stochasticity during the runtime (as opposed to the training time) as well as
the stochasticity of the environment.
% 
These observations indicate that \emph{any} symbol grounding system potentially suffers from 
the symbol stability problem.
% 
As for the stochasticity of the environment,
in many real-world tasks, it is common to obtain stochastic observations
due to the external interference, e.g., vibrations of the camera caused by the wind.
% 
As for the stochasticity of the network,
both
VAEs \cite{kingma2013auto,jang2016categorical,higgins2016beta} used in Latplan
and
GANs (Generative Adversarial Networks) \cite{goodfellow2014generative} used in Causal InfoGAN \cite{kurutach2018learning}
rely on sampling processes.
% , including VAEs
% \cite{kingma2013auto,jang2016categorical,higgins2016beta}
% and Generative Adversarial Networks \cite[GAN]{goodfellow2014generative}.
% Latplan \cite{Asai2018} uses VAE and Causal InfoGAN \cite{kurutach2018learning} uses InfoGAN.

\section{Analyzing the State Autoencoder}
\label{analysis}

To obtain a deeper understanding of the mechanism that generates
unstable symbols in vanilla SAEs, we
analyzed its mathematical model and the public source code of Latplan (commit d48ee62).
% 
We found that the source code differs from the original mathematical formulation of Gumbel-Softmax VAE (GS-VAE)
in that they are using an alternative loss function,
and we found that \emph{this very change} turned out to be essential to the success of their experiments
by suppressing the instability of the propositions.
In the following, we illustrate our finding using the idealized case where the encoder is the Bernoulli distribution,
although in reality, it is approximated by a Gumbel-softmax distribution.

% % Keep in mind that the reviewers might be symbolic people -- so use the intuitive words as possible.
% The purpose of GS-VAE is to obtain a NN
% whose latent layer resembles a random categorical distribution
% with a certain number of categories $M$ ($M$=2 in Latplan).

Given a dataset $\mathcal{X}=\{\mathbf{x}^{(1)},\dots,\mathbf{x}^{(I)}\}$,
let $q(\mathbf{b} \mid \mathbf{x})$ and $p(\mathbf{x} \mid \mathbf{b})$ be the probabilities that the encoder and the decoder of GS-VAE respectively outputs the value $\mathbf{b}$ given $\mathbf{x}$ and $\mathbf{x}$ given $\mathbf{b}$,
where $\mathbf{x}\in \mathcal{X}$ corresponds to a visual observation of the environment, and $\mathbf{b}\in\{0,1\}^N$ corresponds to its latent representation.
In principle, GS-VAE is trained by minimizing the following objective function with respect to $p(\mathbf{x} \mid \mathbf{b})$ and $q(\mathbf{b}\mid \mathbf{x})$:
{
% From ``2019 Formatting Instructions'': For mathematical environments, you may reduce fontsize but not below 6.5 point.
\relsize{-1}
\begin{align}
\label{vae-obj} \sum_{\mathbf{x}\in \mathcal{X}} \left(\mathbb{E}_{q(\mathbf{b}\mid \mathbf{x})}\left[-\log p(\mathbf{x}\mid \mathbf{b})\right] + \mathrm{KL}(q(\mathbf{b}\mid \mathbf{x}) \parallel p(\mathbf{b}))\right)
\end{align}
}
\\\noindent where $p(\mathbf{b}) = \prod_{n=1}^N p(b_n) = \prod_{n=1}^N \mathrm{Bern}(0.5)$ is the target, $N$-dimensional Bernoulli distribution with uniform probabilities
% emphasize that this Bernoulli is the target distrib, consistent with the notation p*    ^^^^^^ 
% (``*'' usually means ``goal'' or ``optimal'' also in planning)
% $i$ is the index for each data point, 
and $\mathrm{KL}(q || p)$ represents the Kullback-Leibler divergence from $p$ to $q$.
The first term in Eq.~\eqref{vae-obj} is the reconstruction loss, which measures the quality of the reconstructed data points,
and the second term regularizes the encoder by making the encoder $q(\mathbf{b}\mid \mathbf{x})$ closer to $p(\mathbf{b})$.
The second term is computed as
{
% From ``2019 Formatting Instructions'': For mathematical environments, you may reduce fontsize but not below 6.5 point.
\relsize{-1}
\begin{align*}
 &{\mathrm{KL}}(q(\mathbf{b}\mid \mathbf{x}) \parallel p(\mathbf{b})) \\
= & - \sum_{n=1}^N \sum_{k\in\{0,1\}}q(b_{n}=k \mid \mathbf{x}) \log\frac{p(b_{n}=k)}{q(b_{n}=k \mid \mathbf{x})}\\
=& - \sum_{n=1}^N \sum_{k\in\{0,1\}}q(b_{n}=k \mid \mathbf{x}) \left(\log \frac{1}{2} - \log{q(b_{n}=k \mid \mathbf{x})}\right)\\
=& - \sum_{n=1}^N H_q(b_n\mid \mathbf{x}) + \mathrm{const.} = -H_q(\mathbf{b} \mid \mathbf{x}) + \mathrm{const.},
\end{align*}
}
where $H_q(\mathbf{b}|\mathbf{x})$ is the entropy of $\mathbf{b}$ given $\mathbf{x}$ under $q$.
% We assume every $b_n$ are mutually independent.

\subsubsection{Entropy Regularization in Latplan}

We found that the loss computation in the Latplan code has the \emph{opposite sign} on the KL divergence,
i.e., it is \emph{maximizing} the KL divergence instead of minimizing it.
The system works
because maximizing the KL divergence corresponds to minimizing the entropy of $q$, thus finding a stable representation.
% 
This is natural considering the nature of the original GS-VAE:
The original loss function of the GS-VAE tries to make the latent distribution closer to the fair random Bernoulli distribution $p(\mathbf{b})$
that takes 0 or 1 with the equal probability, i.e., \emph{as random as possible},
which is, in fact, opposite from the concept of stability.
% This is clearly the opposite of the stable symbols we pursue.
Instead, Latplan has a negated loss, which resulted in
maximizing the KL divergence and making the representation \emph{less random}.

The resulting loss function implemented in Latplan is therefore as follows:
{
% From ``2019 Formatting Instructions'': For mathematical environments, you may reduce fontsize but not below 6.5 point.
\relsize{-1}
\begin{align} 
\nonumber  &\brackets{\text{rec loss}}-{\mathrm{KL}}(q||p)                                    \\
\nonumber =&\brackets{\text{rec loss}}+{\mathrm{KL}}(q||p) - 2 {\mathrm{KL}}(q||p)          \\
\label{loss+ent} =&\brackets{\text{\small the original GS VAE loss}}      + 2 H_q(\mathbf{b} \mid \mathbf{x}).
\end{align}
}
Since the entropy measures the randomness of the random variables,
the extra entropy term in Eq.~\eqref{loss+ent} regularizes the network by penalizing the unstable representation.
% 
In the later sections, we empirically show that the original loss function (Eq.~\eqref{vae-obj}) for GS-VAE results in
a much higher instability compared to the GS-VAE with the Entropy regularization (Eq.~\eqref{loss+ent}).
% 
% \emph{It is not our intention to criticize Latplan for its ``incorrect VAE loss''.
% Our claim is that their loss implementation was the key to achieving the desired behavior.}
% In fact, there is no reason that the loss should be identical to the GS-VAE loss.

%% stop claiming this
% Note that reducing $H_q(\mathbf{b}\mid \mathbf{x})$
% reduces only the inherent stochasticity of the network that emerges for the same input $\mathbf{x}$,
% but not the stochasticity originated from the noisy input, which is characterized by $H_q(\mathbf{b})$.

\subsubsection{Removing the Run-Time Stochasticity}
\label{argmax}

Another improvement we made from the original approach in Latplan is that we can
disable the stochasticity of the network while performing the planning.
After the training is finished, we replace the Gumbel-softmax activation with
a pure argmax of class probabilities, which makes the network fully deterministic:
\[
 z_{i} = \textbf{if}\ (i\ \text{is} \arg \max_i (\log \pi_{i}))\ \textbf{then}\ 1\ \text{else}\ 0.
\]
% 
This technique reduces the inherent stochasticity of the network.
%% We actually haven't prove anything about this, so let's remove it for safety
% but does not reduce the stochasticity originated from the noisy input.

\section{Zero-Suppressed State AutoEncoder}
\label{zsae}

In Latplan, Entropy Regularization was the key to address the symbol stability (see \refsec{evaluation})
while it was only accidentally introduced.
In this context, a natural next step toward obtaining the more stable symbols is to introduce a new regularization
for the propositional representation.
% To further address the symbol stability problem in Latplan,
% we need to reduce the effect of the external stochasticity of the environment
% to the latent, propositional representation, while preserving the reconstruction accuracy
%  (\emph{robust performance}).
% We address it by Zero-Suppressed State AutoEncoder (ZSAE),
We propose Zero-Suppressed State AutoEncoder (ZSAE),
an SAE with an additional regularization designed for the discrete representation that we call \emph{zero-suppression}.
% 
Its fundamental idea is to penalize the
true propositions in the latent layer so that no propositions unnecessarily flip to true at random
while preserving the propositions that are absolutely necessary for maintaining the reconstruction accuracy.
% Since Gumbel-Softmax (GS) is basically a Softmax function,
% and SAE uses 2 categories,
% This can be seen as adding an
% asymmetric penalty for a particular class label (true) used in the encoding.
% 
% The actual implementation of Gumbel-Softmax represents a binary variable $b_n$ as
% a one-hot vector of 2 categories, $z_{nk}, k\in\braces{0,1}$,
% where $z_{n0} + z_{n1} = 1$ and $b_n = z_{n1}$.
The resulting loss is 
\emph{asymmetric} to a particular label $k=1$ (true):
% From ``2019 Formatting Instructions'': For mathematical environments, you may reduce fontsize but not below 6.5 point.
\begin{align*}
 % \brackets{\text{loss}} = & \brackets{\text{\small vanilla SAE loss}} + \brackets{\text{\small 0-Sup. loss}} \\ 
 % =                        & \brackets{\text{\small vanilla SAE loss}} + \alpha \sum_n b_n % q(b_n=1 \mid x)
 \brackets{\text{loss}} = & \brackets{\text{\small vanilla SAE loss}} + \alpha \sum_n \sum_{k\not=0} z_{nk} % q(b_n=1 \mid x)
\end{align*}
where $\alpha$ is the magnitude of regularization.
This formulation takes a general form
that also covers the multi-valued SAS+ representation with $k\geq 2$.
In principle, this method could also be used for a SAS+ representation,
but we focus on the binary representation in this paper.

%% This paragraph is unnecessary
% This technique alters $p(\mathbf{b})$, the target distribution of GS-VAE.
% Recall that, in GS-VAE, $p(b_n) = \mathrm{Bern}(0.5)$,
% i.e. the probability for $b_n$ to turn to 1 is a fair coin toss.
% Since zero-suppression penalizes the true bits ($b_n$=1), it changes this fairness: The larger the $\alpha$,
% the less $p(b_n=1)$,
% thus the more propositions take 0 (\refig{zsae-overview}).
% This changes the target distribution of the KL-divergence.

% % Maybe add this in the camera-ready
% Note that this generalizes to a categorical case by penalizing all non-zero values: $\bars{q(b_n \not= 0\mid \mathbf{x})}$

One additional advantage of the ZSAE is that
several neurons are completely deactivated, i.e. they always take the value of zero
and can be pruned afterward to reduce the network size,
similar to Zero-Suppressed Decision Diagrams \cite{minato1993zero}.
Unlike traditional NN compression methods \cite{cheng2017survey}, it does not suffer from
accuracy degradation because the activations are discrete and therefore no additional retraining is required.
In the continuous cases, even the minuscule activations could be amplified by the weights and significantly affect the
later pipelines of the neural networks.
Our method for the discrete representations complements
the prior work for the continuous ones.

Assume the propositional layer $z_{nk}$ is connected to the next layer of $L$ neurons
by a fully-connected network $h_l=\sigma\parens{\sum_{n=1}^N \sum_{k\in\braces{0,1}} W_{nkl}z_{nk}+B_l}$
with weights $W_{nkl}$, biases $B_l$, and a nonlinear activation $\sigma$.
($1\leq l \leq L$.)
When we assume that $z_{n0}=1$ and $z_{n1}=0$ always holds ($b_n=0$ for all inputs),
we can prune the index $n$
by adding $W_{n0l}$ to $B_l$ and removing $W_{nkl}$ for $\forall k\in\braces{0,1}$, $\forall l \in \braces{1,...L}$,
which removes $2L$ float values for each zero-suppressed bit $b_n$.
Therefore, if we remove $\Delta N$ bits from the latent space, it removes $2L\Delta N$ float values from the network.
 % 
We can similarly prune the weights $W'$ from the previous layer of $M$ neurons.
% When $W'=(W'_{mnk})$ and bias $B'=(B'_{nk})$ ($m\in[1..M], n\in[1..N], k\in \braces{0,1}$),
% we remove $2(M+1)$ for each zero-suppressed bit $b_n$.
% Therefore, if we remove $\Delta N$ bits from the latent space, it removes $2(M+1)\Delta N$ float values from the network.

\textbf{Implementation Note.} Regardless of $\alpha$,
the regularization tends to be too strong near the beginning of the training.
In practice, we set $\alpha=0$ until 1/3 of the total epochs.
We also confirmed that gradually increasing $\alpha$ work,
but we did not use it in the later experiments.


\section{Empirical Evaluation}
\label{evaluation}

We evaluated various SAE implementations across 5 different
image domains depicting 8-puzzles or Lights Out puzzle game \cite{lightsout}.
% Experiments were performed on a compute cluster with Xeon Ivy Bridge
% processors and Tesla K40 GPUs.
Each training takes at most 30 minutes.

\textbf{MNIST 8-puzzle}
is an image-based version of the 8-puzzle, where tiles contain hand-written digits (0-9) from the  MNIST database \cite{lecun1998gradient}.
Valid moves in this domain swap the ``0'' tile  with a neighboring tile, i.e., the ``0'' serves as the ``blank'' tile in the classic 8-puzzle. 
The \textbf{Scrambled Photograph 8-puzzle (Mandrill, Spider)} cuts and scrambles real photographs, similar to the puzzles sold in stores).
% These differ from the MNIST 8-puzzle in that ``tiles'' are \textit{not} cleanly separated by black regions
% (\latentplanner has no built-in notion of square or movable region).
% In \textbf{Towers of Hanoi (ToH)},
% we generated the 4 disks instances.
% 4-disk ToH resulted in a 15-step optimal plan.
\textbf{LightsOut} \cite{lightsout} is
a game where a grid of lights is in some on/off configuration ($+$: On),
and pressing a light toggles its state as well as the states of its neighbors.
The goal is all lights Off.
% Unlike previous puzzles, a single operator can flip 5/16 locations at once and
% removes some ``objects'' (lights).
% This demonstrates that \latentplanner is not limited to domains with highly local effects and static objects.
\textbf{Twisted LightsOut} distorts the original LightsOut game image by a swirl effect.
% showing that \latentplanner is not limited to handling rectangular ``objects''/regions.

% There are three versions of 8-puzzles (MNIST, Mandrill, Spider).
% MNIST 8-puzzle is an image-based versions of the 8-puzzle, where tiles contain
% hand-written digits (0-9) from the MNIST database
% \cite{lecun1998gradient}. Each digit is shrunk to 14x14 pixels, so each
% state of the puzzle is a 42x42 image.  Valid moves in this domain swap
% the ``0'' tile with a neighboring tile, i.e., the ``0'' serves as the
% ``blank'' tile in the classic 8-puzzle.  The entire state space consists
% of 362880 states ($9!$) and 967680 actions.  From any specific goal state, the reachable
% number of states is 181440 ($9!/2$).  Note that the same image is used
% for each digit in all states, e.g., the tile for the ``1'' digit is the
% same image in all states.
% 
% Mandrill and Spider are 8-puzzles generated by cutting and scrambling real photographs
% (similar to sliding tile puzzle toys sold in stores). We used the
% ``Mandrill'' and ``Spider'' images, two of the standard benchmark in the image processing
% literature.  The image was first converted to greyscale and then
% % rounded to black/white (0/1) values
% histogram-normalization and contrast enhancement was applied.
% The same number of transitions as in the MNIST-8puzzle experiments are used.
% 
% 
% Lights Out is a puzzle game where a grid of lights is in some on/off configuration ($+$: On),
% and pressing a light toggles its state (On/Off) as well as the state of all of its neighbors.
% The goal is all lights Off.
% The image dimension is 36x36 and the size of each button ($+$ button) is 9x9.
% 4x4 LightsOut has $2^{16}$=65536 states and $16\times 2^{16}$=1048576 transitions.
% Similar to the 8-puzzle instances, we used 20000 transitions.
% Training:validation ratio 9:1 is maintained (i.e. only 36000 images and 18000 transitions are used for training).
% 
% Another version of Lights Out game is Twisted LightsOut.
% While the images have the same structure as LightsOut, 
% we additionally applied a swirl effect available in scikit-image package
% in order to remove the grid-like structure in the images.
% The effect is applied to the center, with strength=3, linear interpolation, 
% and radius equal to 0.75 times the dimension of the image.

% unnecessary?
% Training is performed on 150 epochs, with an annealing schedule
% $\tau=5e^{- (\log\frac{5}{0.7})\frac{t}{150}}\in [0.7,5]$.
% On all configurations, $\alpha$ is initially set to 0 during the first 50 epochs until set to the specified value.
% This is in order to make the network sufficiently trained to
% reconstruct the environment before trying to stabilize the propositions.

\subsection{The Quality of the Latent Representation}

We compare the quality of the latent representation
produced by the ZSAE and the vanilla SAE.

\subsubsection{State Variance}

The first metric we evaluated is the bit-wise variance of the state encoding for the same/similar input,
which directly measures the stability of the representation.
We trained several SAEs for each domain with the different latent layer sizes (numbers of propositions) $N$
and then evaluated the variance.
In all experiments below,
we randomly generated 100 images with a domain-specific generator for each puzzle domain,
then encoded each of them with the SAE 100 times.
We measured the variance of the propositions, i.e. the variance of latent activations (0 or 1)
across 100 encoding trials of the same image.
We then took the mean of the variances over the entire propositions.

We evaluated three versions of the SAE:
(1) NG-SAE, an SAE trained with the original GS-VAE loss function as discussed in \refsec{analysis}, and
(2) Vanilla SAE in the original paper of Latplan \cite{Asai2018} and the Github source code,
(3) Zero-Suppressed SAE (ZSAE).

The first thing we tested is to replace the Gumbel-softmax activation with a deterministic argmax function
after the training (\refsec{argmax}).
In all SAEs, this reduced the variance to 0 for a single input because all networks become deterministic.
We omit the results due to space because it is rather obvious.
In the following experiments, we always replace the activation function with argmax during testing.

We next measured the variance of SAEs in a noisy setting, where
we perturbed the input image by Gaussian noise for each of the 100 trials of the same image.
\reftbl{tab:stability} (first columns) indicates that
the propositions made by NG-SAE are highly random,
while the entropy regularization in the vanilla SAE suppresses the stochastic behavior to some extent.
ZSAE further reduces the variance and achieves the most stable representation.
The effect of SAE$\rightarrow$ZSAE was typically 2-3 and up to 4 orders of magnitude ($2.5e-4\rightarrow 4.5e-8$),
much stronger than that of Entropy Regularization (NGSAE$\rightarrow$SAE, 1 to 2 orders of magnitude).
Due to the poor performance of NG-SAE, we do not study it any further in the later experiments.

\begin{table*}[tbp]
 \centering
 \resizebox{.95\textwidth}{!}{
 \begin{tabular}{|r|*{17}{c|}}
       & \multicolumn{6}{c|}{Mean variance over bits (with noisy images)}
       % & \multicolumn{4}{c|}{True ratio}
       & \multicolumn{5}{c|}{Effective bits}
       & \multicolumn{6}{c|}{Mean Square Error (MSE)}
  \\
$N=$ & \multicolumn{3}{c|}{100} & \multicolumn{3}{c|}{1000}
     % & \multicolumn{2}{c|}{100} & \multicolumn{2}{c|}{1000}
     & \multicolumn{2}{c|}{100} & \multicolumn{2}{c|}{1000}
     % & \multicolumn{3}{c|}{100} & \multicolumn{3}{c|}{1000}
     & Optimal
     & \multicolumn{2}{c|}{100} & \multicolumn{2}{c|}{1000} & \multicolumn{2}{c|}{36}
  \\
domain    & NG-SAE & SAE    & ZSAE            & NG-SAE & SAE    & ZSAE            & SAE & ZSAE        & SAE  & ZSAE & Encoding & SAE     & ZSAE   & SAE    & ZSAE   & SAE    & ZSAE           \\ 
MNIST     & 8.4e-2 & 8.6e-3 & \textbf{3.7e-6} & 5.3e-2 & 2.2e-4 & \textbf{1.1e-7} & 100 & 51          & 1000 & 68   & 18.4     & $<$1e-4 &$<$1e-4 &$<$1e-4 &$<$1e-4 &$<$1e-4 &\uline{9.1e-3}  \\ 
Mandrill  & 1.1e-3 & 8.3e-4 & \textbf{3.0e-5} & 4.2e-4 & 2.5e-4 & \textbf{4.5e-8} & 100 & 46          & 1000 & 182  & 18.4     & 3.0e-4  &2.8e-4  &2.1e-4  &2.3e-4  &2.0e-4  &{3.2e-4}        \\ 
Spider    & 8.5e-4 & 4.9e-4 & \textbf{6.3e-6} & 2.3e-4 & 4.2e-4 & \textbf{7.3e-7} & 100 & 49          & 1000 & 200  & 18.4     & 2.7e-4  &2.2e-4  &3.1e-4  &2.8e-4  &$<$1e-4 &\uline{2.8e-2}  \\ 
L-Out     & 9.0e-3 & 2.0e-4 & \textbf{3.9e-6} & 8.1e-3 & 1.4e-4 & \textbf{7.5e-6} & 100 & \textbf{16} & 1000 & 66   & 16       & $<$1e-4 &$<$1e-4 &$<$1e-4 &$<$1e-4 &2.9e-4  &{2.8e-4}        \\ 
Twisted   & 1.0e-2 & 7.1e-4 & \textbf{5.1e-6} & 1.0e-2 & 4.5e-4 & \textbf{1.6e-7} & 100 & \textbf{16} & 1000 & 49   & 16       & $<$1e-4 &$<$1e-4 &$<$1e-4 &$<$1e-4 &$<$1e-4 &\uline{5.7e-3}  \\ 
\end{tabular}
}
 \caption{\textbf{Representation characteristics.}
Results comparing the NG-SAE, vanilla SAE and ZSAE ($\alpha$=0.7).
(\tb{Left}) Comparing the representation variance over 100 randomly generated images encoded 100 times with Gaussian noise added each time.
(\tb{Middle}) The number of bits that ever turns true when encoding the entire state space.
 In LightsOut and Twisted, ZSAE($N$=100) finds an optimal, 16-bit representation of the 4x4 puzzle.
(\tb{Right}) Mean Square Error for the test data.
 }
\label{tab:stability}
\end{table*}

\subsubsection{Effective Size of the Representation}

Next, \reftbl{tab:stability} (middle columns) shows that the number of effective bits,
i.e. the number of propositions that \emph{ever} change their values over all states, is low in ZSAE, showing that
ZSAE obtained a more compressed, compact representation of the input.
In MNIST, the numbers are comparable between ZSAEs with $N$=100,1000,
which shows that the network is able to find an encoding of almost the same size
regardless of the size of the latent layer (upper bound of the size of
propositions), reducing the need for hyperparameter tuning.
In LightsOut and Twisted, ZSAE even finds the 16bit optimal representation for the 4x4 light grids.
% The additional
% penalty encourages the network to find the more compact representation,
% rather than freely consuming the latent space capacity in an entangled representation.

% Furthermore, while regularization in general tends to trade accuracy and
% the desired characteristics in the activation, we observe that the
% reconstruction loss is still low if you maintain the latent space size
% large enough.

\subsection{Output Accuracy}

Regularization in general works by restricting the neural network
to achieve some desirable property, e.g., most commonly for suppressing the overfitting \cite[chap.5]{Goodfellow-et-al-2016}, but in our case
for improving the stability. Thus, as a result of the restricted expressive capacity, the output accuracy may be degraded
when the regularization is too strong.

Thus we next tested if the zero-suppression affects the output accuracy.
In \reftbl{tab:stability} (right columns)
we show the Mean Square Error between the input and the output
for 100 randomly generated images.
The results indicate that the zero-suppression does not significantly affect the output accuracy for $N$=100,1000.
However, for $N$=36 (a parameter tuned for vanilla SAEs to have the least variance), the zero-suppression
harmed the accuracy because the network is already small and the further penalty affected the training.
In other words, it is better to combine ZSAE with an overcapacity network,
since ZSAE then automatically compresses the representation.
% If the network size is already optimally tuned for vanilla SAE (i.e. $N$=36),
% the additional regularization may trade accuracy for stability.

\subsubsection{Hyperparameter Sensitivity}

\begin{table}[htb]
 \centering
 \resizebox{.95\columnwidth}{!}{
 \begin{tabular}{|r|*{8}{c|}}
     & \multicolumn{8}{c|}{Mean variance over bits (with noisy images)} \\
     & \multicolumn{3}{c|}{SAE} 
     & \multicolumn{2}{c|}{ZSAE($\alpha$=0.7)} 
     & \multicolumn{3}{c|}{ZSAE($N$=100)}
  \\
$N=$      &36     & 100    & 1000   & 100    & 1000   & $\alpha=$0.2 & 0.5    & 0.7    \\
MNIST     &1.8e-4 & 8.6e-3 & 2.2e-4 & 3.7e-6 & 1.1e-7 & 4.5e-7       & 0.0e+0 & 3.7e-6 \\
Mandrill  &2.9e-4 & 8.3e-4 & 2.5e-4 & 3.0e-5 & 4.5e-8 & 2.3e-6       & 3.4e-6 & 3.0e-5 \\
Spider    &5.8e-6 & 4.9e-4 & 4.2e-4 & 6.3e-6 & 7.3e-7 & 4.6e-6       & 8.3e-6 & 6.3e-6 \\
L-Out     &2.5e-6 & 2.0e-4 & 1.4e-4 & 3.9e-6 & 7.5e-6 & 1.1e-4       & 1.5e-5 & 3.9e-6 \\
Twisted   &2.1e-5 & 7.1e-4 & 4.5e-4 & 5.1e-6 & 1.6e-7 & 8.8e-5       & 4.2e-6 & 5.1e-6 \\
\end{tabular}
}
 \caption{Sensitivity of ZSAE to the hyperparameter $\alpha,N$ compared to that of SAE to the hyperparameter $N$.}
 \label{sensitivity}
\end{table}

We tested the sensitivity of ZSAE to its new hyperparameter $\alpha$ (\reftbl{sensitivity}).
The purpose of this experiment is to show that ZSAEs are less sensitive to the choice of both $N$ and $\alpha$,
while vanilla SAEs are sensitive to $N$.
% 
We compared the state variances under noise
between the vanilla SAE and the ZSAE with $\alpha=$0.2,0.5,0.7, $N$=100,1000.
We also added $N$=36 as the best case for the vanilla SAE.
% 
We observed that the ZSAEs achieve the comparable state variance with various $\alpha$ and $N$,
while the vanilla SAEs are significantly affected by $N$.
For instance,
the variance for the vanilla SAE with $N$=100 is up to two orders of magnitude larger than that for the SAE with $N$=36.
In contrast,
the worst case for the ZSAEs is 1.1e-4 on Twisted with $(N,\alpha)$=(100,0.2),
which is still better than the vanilla SAE with the same $N$.
% 
They also have similar reconstruction loss (MSE) and the effective bits.
Therefore, we conclude that while ZSAE introduces an additional parameter,
the selection of $N$ and $\alpha$ is easy compared to the selection of $N$ in SAE.
%%
%% danger --- this might cause additional experiments for running many trials and compute the mean/variance
% While there are indeed some performance difference,
% this is within the fluctuation that occurs in the different training trials,
% as is case with most machine learning algorithms.

\subsection{Planner Performance}

Next, we compared the success ratio of \latentplanner with various parameters.
We tested both AMA$_1$ and AMA$_2$ proposed in \cite{Asai2018} as the Action Model Acquisition (AMA) methods.
% 
Each domain has 60 problem instances each generated by a random walk from
the goal state. 60 instances consist of 30 instances generated by 7-steps random walks
and another 30 by 14 steps. 30 instances consist of 10 instances whose images are corrupted by Gaussian noise,
10 with salt/pepper noise and another 10 without noise.
It is important to note that \emph{the noiseless instances do not have the external stochasticity},
one of two sources of instability.

We first tested AMA$_1$, an oracular, idealistic AMA that does not incorporate machine learning,
and instead generates the entire propositional state transitions from the entire image transitions.
The purpose we test an impractical AMA$_1$ method is
to separate the effect of a better state representation achieved by ZSAE
and that of the learning procedure in AMA$_2$ that learns the state transitions and the action rules.
As a classical planner, we used FastDownward \cite{Helmert04} with \astar, blind heuristics in order to
remove the effects of the heuristic functions. We ran the solver with no runtime/memory restrictions.

The results in \reftbl{tab:ama1} (left) show that
ZSAEs have \textbf{tripled} the score of SAE ($14\rightarrow43, 6\rightarrow48, 6\rightarrow33$)
under external stochasticity (Gaussian noise).
% 
Regarding the hyperparameter sensitivity,
ZSAE showed a robust performance across $N$=36,64,100
while SAE achieves a good performance only in a single parameter $N$=36.
% 
Due to the small size of the state space compared to the usual classical planning benchmark domains,
the search finishes in a fraction of a second, which means \textbf{the failure is due to graph disconnectedness at the initial/goal nodes}.
% emphasized for contrasting it against the later node expansion experiments
ZSAE tends to fail in $N$=36 in LightsOut and Twisted because of the higher reconstruction error
discussed in the previous subsection.
% 
% Improvements were mainly observed in the problems where
% the initial and the goal states are corrupted by noise, supporting our claim that
% zero-suppression makes the latent representation more stable against the external perturbation.

\begin{table*}[tb]
\centering
\resizebox{.95\textwidth}{!}{
\begin{tabular}{|l|*{6}{*{3}{r}|}|*{6}{*{3}{r}|}}
 \hline
 & \multicolumn{18}{c||}{\textbf{AMA$_1$ results}} & \multicolumn{18}{c|}{\textbf{AMA$_2$ results}}
 \\
 & \multicolumn{6}{c|}{Gaussian ($\sigma$=0.6)}
 & \multicolumn{6}{c|}{Salt/Pepper ($p$=0.12)}
 & \multicolumn{6}{c||}{No noise}
 & \multicolumn{6}{c|}{Gaussian ($\sigma$=0.6)}
 & \multicolumn{6}{c|}{Salt/Pepper ($p$=0.12)}
 & \multicolumn{6}{c|}{No noise}
 \\
 & \multicolumn{3}{c|}{SAE} & \multicolumn{3}{c|}{ZSAE}
 & \multicolumn{3}{c|}{SAE} & \multicolumn{3}{c|}{ZSAE}
 & \multicolumn{3}{c|}{SAE} & \multicolumn{3}{c||}{ZSAE}
 & \multicolumn{3}{c|}{SAE} & \multicolumn{3}{c|}{ZSAE}
 & \multicolumn{3}{c|}{SAE} & \multicolumn{3}{c|}{ZSAE}
 & \multicolumn{3}{c|}{SAE} & \multicolumn{3}{c|}{ZSAE}
 \\
 % 
$N=$
 & {36} & {64} & {100}  & {36} & {64} & {100} 
 & {36} & {64} & {100}  & {36} & {64} & {100} 
 & {36} & {64} & {100}  & {36} & {64} & {100} 
 & {36} & {64} & {100}  & {36} & {64} & {100} 
 & {36} & {64} & {100}  & {36} & {64} & {100} 
 & {36} & {64} & {100}  & {36} & {64} & {100} 
 \\
\hline
%% high-noise results (ama1-argmax-highnoise)
MNIST          & 0       & 0  & 0       & \tb{17} & 0       & 0       & 17      & 5  & 0       & \tb{20} & \tb{10} & \tb{17} & 20  & 20  & 20      & 20      & 20      & 20  
               & 8       & 8  & 4       & \tb{20} & \tb{14} & \tb{10} & 18      & 6  & 0       & \tb{20} & \tb{9}  & \tb{17} & 18  & 13  & 5       & \tb{20} & \tb{19} & \tb{18} \\
Mandrill       & \tb{4}  & 3  & 0       & 0       & \tb{6}  & \tb{5}  & 16      & 17 & 8       & \tb{20} & \tb{20} & \tb{20} & 20  & 20  & 20      & 20      & 20      & 20  
               & \tb{13} & 10 & 3       & 4       & \tb{13} & \tb{12} & 18      & 13 & 6       & 18      & \tb{19} & \tb{19} & 18  & 13  & 10      & 18      & \tb{19} & \tb{20} \\
Spider         & 8       & 3  & \tb{6}  & \tb{12} & 3       & 5       & 20      & 7  & 16      & 20      & \tb{20} & \tb{20} & 20  & 20  & 20      & 20      & 20      & 20  
               & 13      & 11 & \tb{17} & \tb{17} & \tb{18} & 16      & 17      & 10 & 15      & \tb{18} & \tb{20} & \tb{18} & 17  & 14  & 15      & 17      & \tb{20} & \tb{18} \\
L-Out          & 1       & 0  & 0       & \tb{10} & \tb{19} & \tb{20} & \tb{20} & 13 & 18      & 12      & \tb{19} & \tb{20} & 20  & 20  & 20      & 20      & 20      & 20  
               & 10      & 0  & 0       & \tb{20} & \tb{20} & \tb{12} & 20      & 20 & \tb{20} & 20      & 20      & 18      & 20  & 19  & \tb{20} & 20      & \tb{20} & 18 \\
Twisted        & 1       & 0  & 0       & \tb{4}  & \tb{20} & \tb{3}  & \tb{17} & 12 & 6       & 12      & \tb{20} & \tb{17} & 20  & 20  & 20      & 20      & 20      & 20  
               & 3       & 0  & 0       & \tb{10} & \tb{16} & \tb{12} & 19      & 17 & \tb{20} & \tb{20} & \tb{20} & 15      & 20  & 20  & \tb{20} & 20      & 20      & 15 \\ \hline
\textbf{Total} & 14      & 6  & 6       & \tb{43} & \tb{48} & \tb{33} & \tb{90} & 54 & 48      & 84      & \tb{89} & \tb{94} & 100 & 100 & 100     & 100     & 100     & 100 
               & 47      & 29 & 24      & \tb{71} & \tb{81} & \tb{62} & 92      & 66 & 61      & \tb{96} & \tb{88} & \tb{87} & 93  & 79  & 70      & \tb{95} & \tb{98} & \tb{89} \\
\hline
%% additional high-noise results (ama1-argmax-highnoise-alpha + ama2-argmax-highnoise-alpha-retake. Mind that the results are 180-sec capped (which I almost forgot).)
\multicolumn{4}{|l|}{Total (ZSAE, $\alpha$=0.2)} & \tb{21} & \tb{10} & 2       & & & & 75 & \tb{88} & \tb{68} & & & & 100 & 100     & 99  &
\multicolumn{3}{|l|}{}                           & 46      & 29      & \tb{33} & & & & 76 & \tb{72} & \tb{71} & & & & 76  & \tb{86} & \tb{81} \\
\multicolumn{4}{|l|}{Total (ZSAE, $\alpha$=0.5)} & \tb{15} & \tb{33} & \tb{12} & & & & 77 & \tb{94} & \tb{89} & & & & 100 & 100     & 100 &
\multicolumn{3}{|l|}{}                           & 36      & \tb{41} & \tb{27} & & & & 56 & \tb{81} & \tb{65} & & & & 59  & \tb{92} & \tb{72} \\
\hline
\end{tabular}
}
\caption{
\textbf{Planning Results.}
(\textbf{Left})
The number of instances successfully solved by Latplan using AMA$_1$ (oracular method)
for comparing the performance of Z/SAE.
Better results among the same configuration of ZSAE/SAE are highlighted in \tb{bold}.
SAEs degrade performance as the surplus capacity produces more unstable propositions
and is better than ZSAE only when tuned to $N$=36.
ZSAEs are robust on the different $N$ and tend to solve more problems than the vanilla SAEs.
(\textbf{Right})
The numbers of instances solved under 180 sec using AMA$_2$ unsupervised learning method for Action Model Acquisition.
% Same highlightation rule as \reftbl{tab:ama1} is applied.
Results indicate that ZSAEs are robust on the different hyperparameters and tend to achieve better performance than vanilla SAEs.
}
\label{tab:ama1}
\label{tab:ama2}
\end{table*}



Next, we compare the planning performance of Z/SAE with AMA$_2$,
a NN-based AMA model that learns from the example state transitions.
It consists of two networks:
(1) Action AutoEncoder (AAE), an autoencoder that learns to cluster the state transitions into a finite number of action labels.
It learns to reconstruct the input propositional successor state $t$ in the output $\hat{t}$.
Its latent layer represents an action label $a$, which is a single Gumbel-Softmax-activated unit of $A$ categories,
where $A$ is the upper bound of the number of action labels.
In addition, every layer of AAE is concatenated with the propositional vector of the current state $s$ as the \emph{secondary} input,
which turns the standard AE formulation of $f(t)=a,\ g(a)=\hat{t}$ into $f(t,s)=a,\ g(a,s)=\hat{t}$,
where $g$ can be interpreted as a function that \emph{applies} an action $a$ to $s$ and obtains $\hat{t}$.
(2) Action Discriminator (AD), a binary classifier modeling the action precondition, which takes $(s,t)$ and returns a boolean.
Combining AAE and AD yields a successor function that can be used for graph search algorithms.
Networks in AMA$_2$ are trained with the same hyperparameters used in the original paper \cite{Asai2018}.
% Similar results were obtained; 
\reftbl{tab:ama2} (right) shows that ZSAEs \textbf{doubled} ($29\rightarrow81, 24\rightarrow62$) the success ratio over vanilla SAEs
under noise.
The gap is not as large in the noiseless scenario, but note that
the scenario is unrealistic: Virtually all real-world data contain various forms of noise/perturbation.

Finally, we compared the search statistics between SAE+AMA$_2$ and ZSAE+AMA$_2$
in order to measure another harmful effect of the unstable propositions
that they \textbf{confuse the duplicate detection of search algorithms and increase the search effort}.
% 
This effect cannot be measured with AMA$_1$
because it creates PDDL/SAS instances based on the fixed set of
input images and both SAE/ZSAE are deterministic (using argmax),
therefore the search graph contains a single node for a single image.
This is not the case with AMA$_2$ because the NNs generate the successor states on the fly.
% (which might not be included in the training examples)
% 
% To demonstrate that this effect is also reduced by ZSAE,
We measured the number of node expansions (left) and the runtime (right)
on the problems successfully solved by both SAE+AMA$_2$ and ZSAE+AMA$_2$ (\refig{fig:ama2-statistics}).
% 
In order to gather the sufficient number of data points for comparing the search statistics of SAE and ZSAE,
we extended the maximum runtime limit of 1 hour
and reduced the input noise so that SAE can solve a sufficient number of problem instances.
The parameters for the Gaussian noise and the salt/pepper noise applied to the input
are $\sigma=0.3$ and $p=0.06$, respectively, compared to $\sigma=0.6$ and $p=0.12$ in \reftbl{tab:ama2}.
In this setting, the gap between ZSAE and SAE narrowed: ZSAE solved 266 instances and SAE solved 262 instances in total.
% 
The plots support our claim that
the randomness in the state encoding of vanilla SAE confuses the duplicate detection and
increase the search effort.
% 
ZSAE resulted in a smaller node expansion in 172 out of 238 instances solved by both SAE and ZSAE,
and in a shorter runtime in 191 out of 238 instances.
% ???
% Since the planner uses a blind search and the problem is unit-cost, all nodes under the solution depth are generated
% (note: mind the difference between evaluation, expansion, generation).

% (:TYPE "exp" :ZSAE-WIN 172 :SAE-WIN 66)
% (:TYPE "time" :ZSAE-WIN 191 :SAE-WIN 47)


\begin{figure}[tb]
 \centering
 \includegraphics[width=0.49\linewidth]{img/static/exp.pdf}
 \includegraphics[width=0.49\linewidth]{img/static/time.pdf}
 \caption{
Double-logarithmic plot of the number of states expanded (left) / time spent (right) by
\astar with goal-count heuristics,
for instances successfully solved under 1 hour by both SAE+AMA$_2$ and ZSAE($\alpha$=0.7)+AMA$_2$, both with $N$=100, all domains.
In both figures,
$x$-axes represent the SAE and $y$-axes represent the ZSAE.
Unsolved instances are shown on the borders.
% , indicating planning with vanilla SAE fails more often than with ZSAE.
% 
We observe that the search effort tends to be larger with the SAE than with the ZSAE.
% While there is a smaller number of opposite cases, it is natural because
Opposite cases are generated due to
the tie-breaking difference and
the random disconnectedness caused by the SAE.
}
 \label{fig:ama2-statistics}
\end{figure}


\subsection{Pruning Inactive Nodes from ZSAE}

% [too obvious and intuitive?]

We discuss the amount of memory reduction possible by the pruning on the nodes
that have constant activations of 0. As we saw from \reftbl{tab:stability},
vanilla SAEs do not have such propositions (all bits are effective).
% While we did not actually implemented the node pruning,

% First, since our network has a fully-connected network between the
% latent layer and the convolutional layer before it, the node reduction
% amounts to a linear weight reduction. 
Representing a fully-connected
network between two layers of $L$ and $N$ nodes requires $(L+1)N$
weights ($+1$ for the bias).
In the network we used,
both the previous and the succeeding layer of the latent propositional layer have 1000 nodes.
In Gumbel-softmax, each proposition corresponds to 2 neurons.
Thus, in the case of ZSAE with $N$=1000 applied to MNIST puzzles,
the representation is compressed down to 68 effective bits and
the weights are reduced by $(1000+1)\times (2\cdot 1000 - 2\cdot 68)$=1865864 for the previous layer,
and $((2\cdot 1000+1)-(2\cdot 68+1))\times 1000 $=1864000 for the succeeding layer.
This number is huge compared to the convolutional weights (3x3, 16 channels, thus 144 float values each) in the upper layers.
The total number of weights in the network is reduced from 8376278 to 5578414 (44\% reduction).
% The saved hdf5 files for the network also reduced from 14MB to XMB.
We do not show the entire results because the results are straightforward:
The numbers can be similarly calculated from the numbers on \reftbl{tab:stability}.

% [wrong! convolutions are more expensive]
% Second, we considered the inference time by the number of multiplication operations and also by the actual test.
% First, the operations: In the same example above,
% matrix multiplication $y=Wx+b$ (where $W$ is the weights and $b$ the bias) requires $LN$ multiplication and $L+N$ additions.
% Moreover, the Gumbel-Softmax operation requires $N$ softmax calculations, gumbel sampling (two logarithms) and one float division.
% This overall amounts to a linear speed up along the reduction of $N$, 
% since the cost of addition is negligible to the cost of multiplication.

\section{Related Work}

% Saying ``similarities'': novelty in disguise.
The proposed method has some similarities to the common technique called $\ell_1$ regularization \cite[Sparse AE]{Goodfellow-et-al-2016},
which is applied to the continuous activations of the neurons or the training weights
and achieves a sparse representation where the continuous values become closer to zero.
$\ell_1$ regularization is applied to all neurons in the same layer
while the proposed zero-suppression is applied to a subset of neurons representing a particular discrete value.
% 
Moreover, regularization is traditionally applied in order to suppress overfitting and improve predictive performance
while our focus is rather on the stability of the representation.
% Unlike the traditional regularization methods,
% regularization is applied on the activations rather than the training weights.
% Unlike the Sparse AutoEncoder \cite{} that uniformly applies $\ell_1$ norm on the hidden continuous activation values,
% zero-suppression is specifically designed for the categorical representation, resulting in a loss that is assymmetric to the class labels.
% Gumbel-softmax is able to approximate not only the binary representation but also the multi-valued representation.
% 
% The technical novelty of the method is in its implementation adapted for Gumbel-Softmax output,
% which is a collection of one-hot vectors $z_{nk} (n\in [1..N], k\in \braces{0,1})$
% where $\forall n; z_{n0} + z_{n1} = 1$ and  $b_n = z_{n1}$.

\input{unused/closed-world.tex}



The search space generated by \latentplanner was shown to be compatible
to an existing Goal Recognition system \cite{amado2018goal,amado2018goalb}.
% 
Another recent approach replacing SAE/AMA$_2$ with InfoGAN \cite{kurutach2018learning}
has no explicit mechanism for improving the stability of the binary representation.
% while it relies on a sampling-based generative method
% It also lacks the mechanism for finding a finite, discrete set of action symbols (in Latplan, done by AMA$_2$),
% thus does not guarantee to enumerate all successors,
% hence its successor generation relies on sampling.
% 
% From latplan paper background
% 
Other methods for generating a symbolic model from the environment \cite{YangWJ07,CresswellMW13,MouraoZPS12}
require symbolic or near-symbolic, structured inputs.
% 
% Framer \cite{lindsay2017framer} parses natural language texts and emits PDDL,
% but requires a clear grammatical structure and word consistency. %, i.e. more symbolic.
% 
Konidaris et al. (\citeyear{KonidarisKL18}) generates a symbolic state space from
the low-level sensor inputs but requires the high-level action symbols.
% 
% while Latplan finds both the symbolic state space 
% and the high-level action symbols by itself, using NN-based machine learning methods,
% SAE and AMA$_2$, respectively.
% 
% The fact that all of these methods require the (near) symbolic inputs indicates
% that their usability will be increased by our approach.
Our approach complements the usability of these approaches by providing more stable symbols.

% \citeauthor{KonidarisKL18} generated a symbolic state space from
% % a low-level, sensor actuator space of an agent characterized as a
% semi-MDP (\citeyear{KonidarisKL18}).
% The semi-MDP input $\brackets{S,O,R,P}$ consists of a state space $S$, a finite set of \emph{options} $O$,
% the reward $R$ and the probability $P$ of executing an option $o\in O$.
% Options in semi-MDP are ``temporally extended actions''\shortcite{KonidarisKL14},
% and are analogous to actions in planning.
% 
% They convert a probabilistic \emph{model} into a propositional \emph{model},
% i.e., they do not generate a model from unstructured inputs.
% In fact, options ($\approx$ actions) in their semi-MDP have names assigned by a human (\pddl{move}/\pddl{interact}),
% and state variables are identifiable entities
% (x/y distances toward objects, light level, state of a switch) i.e. already symbolic.
% For example, in their experimental evaluation,
% $S$ has 33 low-level variables representing
% structured sensor input (e.g., x/y distances between each effector and each object, light level)
% and categorical states (the on/off state of a button, whether the monkey has cried out)
% while there are with little or no entanglements between sensors.

%% this could be in the action symbol grounding paper.
% In various model-free Reinforcement Learning settings such as DQN
% \cite{dqn}, the system avoids the symbol grounding problem by directly
% using the subsymbolic state encoding obtained from a NN.
% However, in these systems, typically the number of the actions and their labels are known \emph{apriori}:
% For example, DQN knows that there are 8-directional joystick and a button.

Previous work in Learning from Observation \cite{BarbuNS10,Kaiser12},
which could produce propositions from the observations (unstructured input)
with the help of hand-coded symbol extractors (e.g. \emph{ellipse detectors} for tic-tac-toe),
typically assume 
% domain-dependent hand-coded symbol extractors,
% such as \emph{ellipse detectors} for tic-tac-toe board data
% which immediately obtains propositions \cite{BarbuNS10}.
% \citeauthor{Kaiser12} (\citeyear{Kaiser12}) similarly assumes grids and pieces
% to obtain the relational structures in the board image.
% These scenarios typically assume
a perfect indoor environment with little noisy interference,
therefore do not have to address the stability of the symbols.



% from latplan paper Related Work

% This section reviews and compares previous work with our proposed architecture.
% A more complete, recent survey of other machine learning methods for planning can be found in \cite{JimenezRFFB12}.

% Compared to the work by \citeauthor{KonidarisKL14} (\citeyear{KonidarisKL14}),
% the inputs to \latentplanner are unstructured (42x42=1764-dimensional arrays for 8-puzzle);
% each pixel does not carry a meaning and the boundary between ``identifiable entities'' is unknown.
% % vs. 33 symbolic continuous/discrete variables in their work
% % Symbols found by the SAE are formed by the entanglements among multiple pixels which are hard to model explicitly.
% % The only specification to the propositional symbols are the upper bound of the number of state variables.
% % SAE thus correctly addresses the grounding of propositional symbols.
% Also, AMA$_2$ automatically grounds action symbols, while they rely on human-assigned symbols (\pddl{move, interact}).
% Furthermore, they do not explicitly deal with robustness to noisy input, while we implemented SAE as a denoising AE.
% % and further generalization can be expected from deep learning techniques (e.g. translational/rotational invariance).
% However, effects/preconditions in AMA$_2$ is implicit in the network, and their approach could be utilized to extract PDDL from AAE/AD (future work).

% Our approach differs from the
% work on learning from observation (LfO) in the robotics literature \cite{ArgallCVB09} in that:
% (1) \latentplanner is trained based on image pairs showing before/after images of valid individual actions, while LfO work is largely based on longer sequence of actions (e.g., of videos);
% (2) \latentplanner generates PDDL suited for high-level (puzzle-like) tasks, while LfO focuses on motion planning tasks. 
%
% A closely related line of work in LfO is learning of board game play from observation of video/images \cite{BarbuNS10,Kaiser12,kirk2016learning}.
% These work made relatively strong assumptions about the environment, e.g., that there is a grid-like environment with ``piece''-like objects.
% In contrast, as shown in \refsec{sec:experiments}, \latentplanner does not make assumptions about the contents of the images.

% There is a large body of work using NNs to directly solve combinatorial tasks,
% starting with the well-known TSP solver \cite{hopfield1985neural}.
% % With respect to state-space search similar to what we consider,
% Neurosolver represents a search state as a node in NN 
% and solved ToH \cite{bieszczad2015neurosolver}. %bieszczad1998neurosolver --- not much space in page 8
% However, they assume a symbolic input.
% \begin{comment}
% %very short version of related work on NN+search
% Previous work combining NNs and symbolic search algorithms embedded NNs {\it inside} a search algorithm
% to provide search control knowledge \cite{alphago,ArfaeeZH11} % TODO: add ,SatzgerK13 later in longer version.
% In contrast, we use a NN-based SAE for symbol grounding, not for search control.
% \end{comment}

% Previous work combining symbolic search and NNs embedded NNs {\it inside} a search
% to provide the search control knowledge,
% % AlphaGo  uses a NN (learned from traces of expert Go players as well as self-play) as a heuristic evaluation function
% % to guide search in a Monte-Carlo tree search algorithm \cite{alphago}.
% e.g., domain-specific heuristic functions for
% the sliding-tile puzzle and Rubik's Cube \cite{ArfaeeZH11},
% classical planning \cite{SatzgerK13},
% or the game of Go \cite{alphago}.
% % We use NNs for symbol grounding/AMA, not for search control.
% % We do not use NNs for search control.
% %ALE lipovetzky
% % -- input is raw state vector, which is  ``unstructured'', but arguable whether it is   ``subsymbolic'' .
% % must be extremely careful in pointing out limitations of DRL because many people are interested in it 
% Deep Reinforcement Learning (DRL) has solved complex problems,
% including video games where it communicates to a simulator through images \cite[DQN]{dqn}.
% %
% In contrast, \latentplanner only requires a set of unlabeled image pairs (transitions), and 
% does not require a reward function for unit-action-cost planning,
% nor expert solution traces (AlphaGo),
% nor a  simulator (DQN), nor predetermined action symbols (``hands'', control levers/buttons).
% % Access to expert traces or simulators is a significant limitation of RL.
% % as they may not be available nor be obtained by random exploration.
% % In contrast, the only requirement of \latentplanner is to collect the transition data which are not necessarily expert or ``good'' in any sense, and can be collected mechanically by a random walk.
% % Finally, RL lacks the completeness/optimality guaranteed by classical planning, which is mandatory for critical applications.
% Extending \latentplanner to symbolic POMDP planning is an interesting avenue for future work.

%  An Autoencoder is a nonlinear generalization of Principal Component Analysis
%  \cite{bourlard1988auto}.
%  Therefore, our approach is somewhat similar to an approach that uses PCA
%  to run RL in the continuous latent space of the configuration space of a high-DOF
%  robot \cite{luck2014latent}.
%  Since the latent representation is more compact than the original configuration space of a robot, they can run reinforcement learning more efficiently.

% PAIR community (Plan/Activitity/Intent Recognition) work. -- peripherally related

% A significant difference between \latentplanner and learning from observation (LfO) in the robotics literature \cite{ArgallCVB09} is that
% \latentplanner is trained based on individual transitions
% while LfO work is largely based on the longer sequence of transitions (e.g. videos)
% and should identify the start/end of actions (\emph{action segmentation}).
% % 
% Action segmentation would not be an issue in 
% an implementation of autonomous \latentplanner-based agent
% because it has the full control over its low-level actuators and
% initiates/terminates its own action for the data collection.



\section{Conclusion}
\label{conclusion}

In this paper, we provided a formal analysis of the State AutoEncoder
(SAE) neural network (NN) in \latentplanner \cite{Asai2018} to
understand its success and 
the issue of its \emph{unstable propositions}.
In the analysis, we identified that the SAE accidentally
uses a different formulation of Gumbel-Softmax VAE \cite{jang2016categorical}
that we named \emph{Entropy Regularization} which stabilizes the truth values.

To further improve the stability,
we introduced Zero-Suppressed State AutoEncoder (ZSAE) which
improves the vanilla SAE by
minimizing the number of true propositions in the representation.
% 
ZSAE improves the success rate and the efficiency of planning performed on
the generated state space and
also removes the need for aggressive hyperparameter tuning.
% despite introducing an additional hyperparameter $\alpha$.
% 
Moreover, 
ZSAE makes it possible to prune some neurons without accuracy degradation
when their activations are constantly zero, % due to discreteness
similar to Zero-Suppressed Decision Diagrams \cite{minato1993zero} and
the knowledge bases with closed world assumption \cite{reiter1981closed}.
% ?
% An interesting avenue for future work is to 
% find an online, adaptive tuning of $\alpha$, the hyperparameter for the penalty.

As a meta-level contribution,
we generalized the problematic behavior of the unstable propositions
into a \emph{Symbol Stability Problem} (SSP), a subproblem of symbol grounding.
% 
We identified two sources of stochasticity which can introduce the instability:
The inherent stochasticity of the network and
the external stochasticity from the observations.
% 
This suggests that
SSP is an important problem that applies to any modern NN-based symbol grounding process
that operates on the noisy real-world inputs and
performs a sampling-based, stochastic process (e.g. VAEs, GANs) that are gaining popularity in the literature.
Thus, characterizing the aspect of SSP would help the process of designing a planning system operated on the real world input.
An interesting avenue for future work is to extend our approach to InfoGAN-based
discrete representation of the environment \cite{kurutach2018learning}.
