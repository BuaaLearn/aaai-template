%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{comment}

* Maintain 80 characters / line.
 
* too much ``''s make the sentence look scattered and visually less recognizable. ``e.g.'' also.

* \em, \bf, \it are all obsolete \TeX primitives, and it does not take effect properly --- for example, {\bf {\it aaa}} shows ``aaa'' in italic but NOT IN BOLD. Use \emph{}, \textit{}, \textbf{} and so on.

* always use \ff, \fd, \cea, \pr, \mv , and do not use it directly, e.g. FF, FD/LAMA2011, etc. 

* use of footnotes should be minimized.

* IPC2011 should always be \ipc . The definition can later be modified in abbrev.sty .

* prefer separated words over hyphened words. domain
  independent>domain-independent, planner independent >
  planner-independent.

* Table, Figure, Fig., should not be used directly. Always use \refig and \reftbl. When the development flag is enabled, direct use of \ref signals an error.

* Caption ends with a period.
\end{comment}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{abstract}
As I noted in the appendix of Latplan paper (arxiv version), 
some of the propositiosn in the latent layer may not
carry significant meaning / affect the output.
While this is not problematic for encoding/decoding, from search/planning perspective this is a major problem,
because this means that a state can have multiple propositional representation.
One effective way to suppress this behavior is to have an additional regularization
for the latent propositions which guides the training toward sparser, disentangled propositional representation,
where all unused propositions converges to 0.
We empirically show that the resulting zero-suppressed SAE
has lower variance encoding for each single state
and improves the performance and the success rate of \latentplanner.
% XXX TODO: unknown if it is possible
Furthermore, we show that this Zero-Suppressed SAE can act like a knowledge base
where you assign a new declarative knowledge in the unused propositions.
We show this by retraining the network using the mixture of the existing dataset and the new dataset,
where two datasets depict different domains, and demonstraining that
the same network can encode the both environments.
\end{abstract}

\section{Introduction}
\subsubparagraph{hidden topic}

BBB! \refig{fig:ip} \cite{Asai2016}

\begin{figure}[tb]
 \includegraphics{img/static/ip.png}
 \caption{This is Invasion Percolation}
 \label{fig:ip}
\end{figure}

\lmcut, \mands, \pdb, \ff, \ce, \cg, \ad, \lc heuristics.

In math mode,

\[
 \lmcut, \mands, \pdb, \ff, \ce, \cg, \ad, \lc.
\]

\lmcuto, \mandso, \ffo, \ceo, \cgo, \ado, \gco, \lco heuristics.

In math mode,

\[
 \lmcuto, \mandso, \ffo, \ceo, \cgo, \ado, \gco, \lco.
\]

\begin{minted}{common-lisp}
(defun factorial (n)
  (if (zerop n)
      1
      (* n (factorial (1- n)))))
\end{minted}

